{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 퍼셉트론\n",
    "- iris data로 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "iris = load_iris()\n",
    "print(iris.feature_names)\n",
    "X = iris['data'][:,(2,3)] # 꽃잎의 길이와 너비\n",
    "y = (iris['target']==0).astype(np.int8) # Iris Setosa인지\n",
    "\n",
    "per_clf = Perceptron()\n",
    "per_clf.fit(X,y)\n",
    "\n",
    "y_pred = per_clf.predict([[2,0.5]])\n",
    "print(y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다층 퍼셉트론 MLP\n",
    "- 은닉층의 가중치를 랜덤하게 초기화 하는 것이 중요!\n",
    "    - IF 모두 0으로 초기화했다면 모든 뉴런이 완전히 같아짐\n",
    "    - 역전파단계에서 각 뉴런이 오차에 얼마나 기여했는지에 따라 업데이트하므로\n",
    "    - 같은 값으로 초기화한다면 모델은 하나의 뉴런이 있는 것처럼 동작할것임<br/><br/>\n",
    "- 활성화함수 계단함수 -> 시그모이드\n",
    "    - 기존의 퍼셉트론의 활성화함수 계단함수로는 역전파를 계산할 수 없음(미분 불능)<br/><br/>\n",
    "- 활성화 함수의 중요성!\n",
    "    - 퍼셉트론은 선형함수 + 활성화 함수임\n",
    "    - 선형함수가 아무리 달라도 선형함수->선형함수 = 선형함수임\n",
    "    - 비선형성(활성화 함수)를 추가 함으로써 어떠한 연속함수라도 근사할 수 있음"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fashion_mnist데이터로 MLP 구현 \\<Tensorflow>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -U tensorflow\n",
    "#%pip install -U tensorflow-gpu\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# 데이터셋 적재\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full , y_train_full) , (X_test , y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규화 값을 0 ~ 1사이로 맞추기\n",
    "X_valid , X_train = X_train_full[:5000]/255.0 , X_train_full[5000:]/255.0\n",
    "y_valid , y_train = y_train_full[:5000] , y_train_full[5000:]\n",
    "X_test = X_test/255.0\n",
    "\n",
    "# 클래스 이름 리스트 정의\n",
    "class_names = [\"T-shirt\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle Boot\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두개의 은닉층으로 이루어진 MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_3 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    해당 코드와 아래의 model은 완전 동일 함\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Flatten(input_shape=[28,28]),\n",
    "        keras.layers.Dense(300,activation='relu'),\n",
    "        keras.layers.Dense(100,activation='relu'),\n",
    "        keras.layers.Dense(10,activation='softmax')\n",
    "    ])\n",
    "'''\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28,28])) # input_shape는 배치크기를 제외하고 샘플의 크기\n",
    "model.add(keras.layers.Dense(300,activation='relu',use_bias=True))\n",
    "model.add(keras.layers.Dense(100,activation='relu'))\n",
    "model.add(keras.layers.Dense(10,activation='softmax')) #마지막 층은 출력층으로 10개의 라벨중 하나로 예측해야하므로 softmax\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight의 shape : (784, 300)\n",
      "biases의 shape : (300,)\n",
      "레이어 파라미터 개수 : 235200 + 300 = 235500\n"
     ]
    }
   ],
   "source": [
    "# 히든레이어 Dense(300)\n",
    "# 235500 = 784*300 + 300\n",
    "hidden1 = model.layers[1]\n",
    "weight , biases = hidden1.get_weights()\n",
    "print(\"weight의 shape : {}\".format(weight.shape))\n",
    "print(\"biases의 shape : {}\".format(biases.shape))\n",
    "print(\"레이어 파라미터 개수 : {} + {} = {}\".format(len(weight.reshape(-1,1)),len(biases),len(weight.reshape(-1,1))+len(biases)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "현재는 weight와 biases가 랜덤으로 초기화 되있음"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile : 모델 생성 후 사용할 손실함수와 옵티마이저를 지정해야한다.\n",
    "\n",
    "- 손실함수 : 현재 y-label은 10개로 10개중 하나로 맞추는 모델임\n",
    "    - 각 클래스는 서로 배타적이다.\n",
    "    - 현재는 0~9를 예측해야 하므로 sparse_categorical_crossentropy\n",
    "        - 만약 확률을 예측해야한다면 categorical_crossentropy ex)[0,0,0,1,0,0,0,0,0,0]    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "len(pd.Series(y_train).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.7331 - accuracy: 0.7602 - val_loss: 0.5345 - val_accuracy: 0.8228\n",
      "Epoch 2/40\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4924 - accuracy: 0.8276 - val_loss: 0.4868 - val_accuracy: 0.8272\n",
      "Epoch 3/40\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4463 - accuracy: 0.8437 - val_loss: 0.4095 - val_accuracy: 0.8602\n",
      "Epoch 4/40\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4181 - accuracy: 0.8526 - val_loss: 0.3896 - val_accuracy: 0.8658\n",
      "Epoch 5/40\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3986 - accuracy: 0.8603 - val_loss: 0.4015 - val_accuracy: 0.8600\n",
      "Epoch 6/40\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3826 - accuracy: 0.8646 - val_loss: 0.3836 - val_accuracy: 0.8678\n",
      "Epoch 7/40\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3685 - accuracy: 0.8697 - val_loss: 0.3864 - val_accuracy: 0.8664\n",
      "Epoch 8/40\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3575 - accuracy: 0.8744 - val_loss: 0.3769 - val_accuracy: 0.8676\n",
      "Epoch 9/40\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3471 - accuracy: 0.8770 - val_loss: 0.3494 - val_accuracy: 0.8792\n",
      "Epoch 10/40\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3368 - accuracy: 0.8795 - val_loss: 0.3502 - val_accuracy: 0.8776\n",
      "Epoch 11/40\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3287 - accuracy: 0.8828 - val_loss: 0.3392 - val_accuracy: 0.8798\n",
      "Epoch 12/40\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3213 - accuracy: 0.8848 - val_loss: 0.3349 - val_accuracy: 0.8832\n",
      "Epoch 13/40\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3144 - accuracy: 0.8869 - val_loss: 0.3482 - val_accuracy: 0.8770\n",
      "Epoch 14/40\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3070 - accuracy: 0.8902 - val_loss: 0.3436 - val_accuracy: 0.8818\n",
      "Epoch 15/40\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3001 - accuracy: 0.8927 - val_loss: 0.3286 - val_accuracy: 0.8834\n",
      "Epoch 16/40\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2943 - accuracy: 0.8942 - val_loss: 0.3184 - val_accuracy: 0.8856\n",
      "Epoch 17/40\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2881 - accuracy: 0.8969 - val_loss: 0.3496 - val_accuracy: 0.8786\n",
      "Epoch 18/40\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2822 - accuracy: 0.8981 - val_loss: 0.3262 - val_accuracy: 0.8828\n",
      "Epoch 19/40\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2780 - accuracy: 0.8991 - val_loss: 0.3153 - val_accuracy: 0.8882\n",
      "Epoch 20/40\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2717 - accuracy: 0.9010 - val_loss: 0.3210 - val_accuracy: 0.8832\n",
      "Epoch 21/40\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2677 - accuracy: 0.9035 - val_loss: 0.3318 - val_accuracy: 0.8802\n",
      "Epoch 22/40\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2630 - accuracy: 0.9047 - val_loss: 0.3213 - val_accuracy: 0.8810\n",
      "Epoch 23/40\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2581 - accuracy: 0.9080 - val_loss: 0.3261 - val_accuracy: 0.8792\n",
      "Epoch 24/40\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2543 - accuracy: 0.9084 - val_loss: 0.3102 - val_accuracy: 0.8876\n",
      "Epoch 25/40\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2491 - accuracy: 0.9105 - val_loss: 0.3105 - val_accuracy: 0.8832\n",
      "Epoch 26/40\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2455 - accuracy: 0.9119 - val_loss: 0.3135 - val_accuracy: 0.8866\n",
      "Epoch 27/40\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2412 - accuracy: 0.9125 - val_loss: 0.3070 - val_accuracy: 0.8888\n",
      "Epoch 28/40\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2372 - accuracy: 0.9145 - val_loss: 0.2987 - val_accuracy: 0.8920\n",
      "Epoch 29/40\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2338 - accuracy: 0.9164 - val_loss: 0.3100 - val_accuracy: 0.8870\n",
      "Epoch 30/40\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2301 - accuracy: 0.9159 - val_loss: 0.2960 - val_accuracy: 0.8910\n",
      "Epoch 31/40\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2255 - accuracy: 0.9187 - val_loss: 0.2959 - val_accuracy: 0.8944\n",
      "Epoch 32/40\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2225 - accuracy: 0.9200 - val_loss: 0.2938 - val_accuracy: 0.8922\n",
      "Epoch 33/40\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2191 - accuracy: 0.9213 - val_loss: 0.3003 - val_accuracy: 0.8918\n",
      "Epoch 34/40\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2152 - accuracy: 0.9240 - val_loss: 0.2941 - val_accuracy: 0.8964\n",
      "Epoch 35/40\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2119 - accuracy: 0.9231 - val_loss: 0.2972 - val_accuracy: 0.8904\n",
      "Epoch 36/40\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2087 - accuracy: 0.9261 - val_loss: 0.3009 - val_accuracy: 0.8928\n",
      "Epoch 37/40\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2057 - accuracy: 0.9257 - val_loss: 0.2884 - val_accuracy: 0.8928\n",
      "Epoch 38/40\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2023 - accuracy: 0.9280 - val_loss: 0.3100 - val_accuracy: 0.8956\n",
      "Epoch 39/40\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2002 - accuracy: 0.9289 - val_loss: 0.3002 - val_accuracy: 0.8924\n",
      "Epoch 40/40\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1975 - accuracy: 0.9300 - val_loss: 0.2941 - val_accuracy: 0.8936\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train,epochs=40,validation_data=(X_valid,y_valid))\n",
    "\n",
    "#모델은 자동으로 갱신됨 , 추가로 학습하고 싶다면\n",
    "#model.fit(X_train,y_train,epochs=추가 학습할 에포크,validation_data=(X_valid,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 시각화 \n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1) # 수직축의 범위를 0~1사이로 설정 , gca()는 현재 그래프 가져오기\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.733143</td>\n",
       "      <td>0.760200</td>\n",
       "      <td>0.534466</td>\n",
       "      <td>0.8228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.492441</td>\n",
       "      <td>0.827618</td>\n",
       "      <td>0.486752</td>\n",
       "      <td>0.8272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  accuracy  val_loss  val_accuracy\n",
       "0  0.733143  0.760200  0.534466        0.8228\n",
       "1  0.492441  0.827618  0.486752        0.8272"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.DataFrame(history.history)\n",
    "a.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3441 - accuracy: 0.8835\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.34407901763916016, 0.8834999799728394]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습한 모델 평가하기\n",
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x263b13e15d0>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_test[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "[[2.8500738e-06 3.0683009e-06 7.9630951e-07 9.8287920e-08 1.5421244e-07\n",
      "  9.7470479e-03 7.1225821e-07 1.1162296e-02 3.0281311e-05 9.7905272e-01]]\n",
      "[[0.   0.   0.   0.   0.   0.01 0.   0.01 0.   0.98]]\n",
      "['Ankle Boot']\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test[0].reshape(-1,28,28))\n",
    "print(y_pred)\n",
    "print(y_pred.round(2))\n",
    "print([class_names[idx] for idx in np.argmax(y_pred,axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x263b1515030>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import ImageOps\n",
    "image_sample = Image.open('./sample/ch10_나이키셔츠.jpg')\n",
    "image_sample = image_sample.resize((28,28)).convert('L')\n",
    "image_sample=ImageOps.invert(image_sample)\n",
    "image_sample = np.array(image_sample)/255.0\n",
    "plt.imshow(image_sample, cmap='gray')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "흑백 반전이 안되면 가방으로 분류함 ImageOps.invert()을 통해 흑백 반전을 해줘야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 14ms/step\n",
      "[[1.2877344e-02 1.5926108e-03 2.7905864e-01 4.9764034e-03 2.8460616e-01\n",
      "  7.6334020e-03 3.9570868e-01 1.5747492e-04 1.2706226e-02 6.8300636e-04]]\n",
      "[[0.01 0.   0.28 0.   0.28 0.01 0.4  0.   0.01 0.  ]]\n",
      "['Shirt']\n"
     ]
    }
   ],
   "source": [
    "image_sample = np.array(image_sample).reshape(-1,28,28)\n",
    "y_pred = model.predict(image_sample)\n",
    "print(y_pred)\n",
    "print(y_pred.round(2))\n",
    "print([class_names[idx] for idx in np.argmax(y_pred,axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 캘리포니아 주택 가격 MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full , X_test , y_train_full , y_test = train_test_split(housing.data,housing.target)\n",
    "X_train , X_valid , y_train , y_valid = train_test_split(X_train_full,y_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11610, 8), (3870, 8), (5160, 8))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.fit_transform(X_valid)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "X_train.shape , X_valid.shape , X_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 설계 : 주택 가격을 예측이 목표!\n",
    "- 출력층이 하나면 됨\n",
    "- 잡음이 많으므로 과대적합을 방지하기 위해 은닉층은 하나만"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30,activation='relu',input_shape=X_train.shape[1:]),     # input.shape는 (데이터 개수 , ~ , ~)일 것이므로\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 30)                270       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 301\n",
      "Trainable params: 301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "# 8*30 + 30 + 30*1 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.7813 - val_loss: 0.5066\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 917us/step - loss: 0.5172 - val_loss: 1.4228\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 881us/step - loss: 0.9839 - val_loss: 2.4296\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 869us/step - loss: 0.4699 - val_loss: 2.6338\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 880us/step - loss: 0.4299 - val_loss: 3.1968\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 901us/step - loss: 0.4221 - val_loss: 3.4145\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 873us/step - loss: 0.4034 - val_loss: 3.7490\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 878us/step - loss: 0.3971 - val_loss: 4.3196\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 873us/step - loss: 0.3916 - val_loss: 4.4572\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 865us/step - loss: 0.3902 - val_loss: 5.0150\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 872us/step - loss: 0.3874 - val_loss: 5.3098\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 867us/step - loss: 0.3857 - val_loss: 5.5519\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 873us/step - loss: 0.3942 - val_loss: 5.9579\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 871us/step - loss: 0.3784 - val_loss: 6.2947\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 883us/step - loss: 0.3720 - val_loss: 6.8860\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 861us/step - loss: 0.3683 - val_loss: 6.9756\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 873us/step - loss: 0.3757 - val_loss: 7.5179\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 876us/step - loss: 0.3665 - val_loss: 7.7378\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 871us/step - loss: 0.3693 - val_loss: 7.9416\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 877us/step - loss: 0.3609 - val_loss: 8.4971\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mean_squared_error',optimizer='sgd')\n",
    "history = model.fit(X_train,y_train,epochs=20,validation_data=(X_valid,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 640us/step - loss: 2.2920\n",
      "1/1 [==============================] - 0s 68ms/step\n"
     ]
    }
   ],
   "source": [
    "mse_test = model.evaluate(X_test,y_test)\n",
    "y_pred = model.predict(X_test[:3])              # 새로운 샘플이라고 가정하자 ><"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.292038679122925,\n",
       " array([[2.0180385],\n",
       "        [3.1496906],\n",
       "        [3.5254576]], dtype=float32))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_test , y_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## skip connection 또는 residual connection\n",
    "ResNet 등에서 사용되는 구조로, 이전 층의 출력을 현재 층의 입력에 더하여 정보의 유실을 방지하고, 학습의 안정성을 높이는 효과가 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip connection\n",
    "\n",
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "hidden1 = keras.layers.Dense(30,activation='relu')(input_)\n",
    "hidden2 = keras.layers.Dense(30,activation='relu')(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_,hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.Model(inputs=[input_],outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 8)]          0           []                               \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 30)           270         ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 30)           930         ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 38)           0           ['input_4[0][0]',                \n",
      "                                                                  'dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 1)            39          ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,239\n",
      "Trainable params: 1,239\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.9189 - val_loss: 0.7203\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 935us/step - loss: 0.7839 - val_loss: 0.4445\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 986us/step - loss: 0.4698 - val_loss: 0.4087\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 951us/step - loss: 0.4469 - val_loss: 0.4662\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 959us/step - loss: 0.4303 - val_loss: 0.4511\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 956us/step - loss: 0.4186 - val_loss: 0.5648\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4085 - val_loss: 0.6299\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4163 - val_loss: 0.6736\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4695 - val_loss: 0.8362\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 958us/step - loss: 0.3815 - val_loss: 1.1827\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 941us/step - loss: 0.3801 - val_loss: 1.4023\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 935us/step - loss: 0.3678 - val_loss: 1.7813\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 922us/step - loss: 0.3646 - val_loss: 2.2115\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 923us/step - loss: 0.3667 - val_loss: 2.7362\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 909us/step - loss: 0.3511 - val_loss: 2.8440\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 915us/step - loss: 0.3514 - val_loss: 3.8077\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 908us/step - loss: 0.3733 - val_loss: 3.9594\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 902us/step - loss: 0.3450 - val_loss: 4.4174\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 922us/step - loss: 0.3429 - val_loss: 5.5305\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 979us/step - loss: 0.3533 - val_loss: 5.2993\n",
      "162/162 [==============================] - 0s 640us/step - loss: 1.5772\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mean_squared_error',optimizer='sgd')\n",
    "history = model.fit(X_train,y_train,epochs=20,validation_data=(X_valid,y_valid))\n",
    "mse_test = model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5772494077682495"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# base모델보다 loss가 확연히 줄었다.\n",
    "mse_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " wide_input (InputLayer)        [(None, 5)]          0           []                               \n",
      "                                                                                                  \n",
      " dense_22 (Dense)               (None, 30)           180         ['wide_input[0][0]']             \n",
      "                                                                                                  \n",
      " dense_23 (Dense)               (None, 30)           930         ['dense_22[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate)    (None, 35)           0           ['wide_input[0][0]',             \n",
      "                                                                  'dense_23[0][0]']               \n",
      "                                                                                                  \n",
      " deep_input (InputLayer)        [(None, 6)]          0           []                               \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 1)            36          ['concatenate_9[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,146\n",
      "Trainable params: 1,146\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 입력을 두개로 하는 모델 , 특성 나눈 후 넣기\n",
    "\n",
    "input_A = keras.layers.Input(shape=[5],name='wide_input')\n",
    "input_B = keras.layers.Input(shape=[6],name='deep_input')\n",
    "hidden1 = keras.layers.Dense(30,activation='relu')(input_A)\n",
    "hidden2 = keras.layers.Dense(30,activation='relu')(hidden1)\n",
    "concat = keras.layers.concatenate([input_A,hidden2])\n",
    "output = keras.layers.Dense(1,name='output')(concat)\n",
    "model = keras.Model(inputs=[input_A,input_B],outputs=output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.9475 - val_loss: 0.8147\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7538 - val_loss: 0.7217\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 991us/step - loss: 0.6938 - val_loss: 0.6832\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6612 - val_loss: 0.6544\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6379 - val_loss: 0.6348\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6203 - val_loss: 0.6209\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 987us/step - loss: 0.6083 - val_loss: 0.6095\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5989 - val_loss: 0.6013\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 993us/step - loss: 0.5914 - val_loss: 0.5951\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 986us/step - loss: 0.5858 - val_loss: 0.5897\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 960us/step - loss: 0.5807 - val_loss: 0.5857\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 986us/step - loss: 0.5769 - val_loss: 0.5814\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5731 - val_loss: 0.5783\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5695 - val_loss: 0.5759\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5670 - val_loss: 0.5730\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5642 - val_loss: 0.5701\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 976us/step - loss: 0.5621 - val_loss: 0.5677\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 962us/step - loss: 0.5593 - val_loss: 0.5663\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 973us/step - loss: 0.5573 - val_loss: 0.5640\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 995us/step - loss: 0.5551 - val_loss: 0.5620\n",
      "162/162 [==============================] - 0s 665us/step - loss: 0.6038\n"
     ]
    }
   ],
   "source": [
    "#model.compile(loss='mean_squared_error',optimizer='sgd')\n",
    "model.compile(loss='mse',optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "\n",
    "X_train_A , X_train_B = X_train[:,:5] , X_train[:,2:]\n",
    "X_valid_A , X_valid_B = X_valid[:,:5] , X_valid[:,2:]\n",
    "X_test_A , X_test_B = X_test[:,:5] , X_test[:,2:]\n",
    "\n",
    "# history = model.fit((X_train_A,X_train_B),y_train,epochs=20,validation_data=((X_valid_A,X_valid_B),y_valid))\n",
    "history = model.fit({'wide_input':X_train_A,'deep_input':X_train_B},y_train,epochs=20,validation_data=({'wide_input':X_valid_A,'deep_input':X_valid_B},y_valid))\n",
    "mse_test = model.evaluate((X_test_A,X_test_B),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6038285493850708"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가장 낮은 loss\n",
    "mse_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여러 개의 출력\n",
    "- 예로들어 이미지에서 주요 물체를 분류하는 작업을 할 때 혹은 규제를 목적으로 사용됨\n",
    "    - 하위 layer가 상위 layer를 그대로 출력하나? 너무 의존하나 확인가능\n",
    "    - 얼굴사진으로 다중 작업 분류를 할 때\n",
    "        - 한 출력은 감정 분류\n",
    "        - 한 출력은 안경의 유무 확인\n",
    "- 여러개의 출력을 갖고 싶을 때 사용할 수 있다.\n",
    "- 이때 각 출력은 각자의 손실함수를 가지고 있어야 함!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " wide_input (InputLayer)        [(None, 5)]          0           []                               \n",
      "                                                                                                  \n",
      " dense_32 (Dense)               (None, 30)           180         ['wide_input[0][0]']             \n",
      "                                                                                                  \n",
      " dense_33 (Dense)               (None, 30)           930         ['dense_32[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_14 (Concatenate)   (None, 35)           0           ['wide_input[0][0]',             \n",
      "                                                                  'dense_33[0][0]']               \n",
      "                                                                                                  \n",
      " deep_input (InputLayer)        [(None, 6)]          0           []                               \n",
      "                                                                                                  \n",
      " main_output (Dense)            (None, 1)            36          ['concatenate_14[0][0]']         \n",
      "                                                                                                  \n",
      " aux_output (Dense)             (None, 1)            31          ['dense_33[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,177\n",
      "Trainable params: 1,177\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_A = keras.layers.Input(shape=[5],name='wide_input')\n",
    "input_B = keras.layers.Input(shape=[6],name='deep_input')\n",
    "hidden1 = keras.layers.Dense(30,activation='relu')(input_A)\n",
    "hidden2 = keras.layers.Dense(30,activation='relu')(hidden1)\n",
    "concat = keras.layers.concatenate([input_A,hidden2])\n",
    "output = keras.layers.Dense(1,name='main_output')(concat)\n",
    "aux_output = keras.layers.Dense(1,name='aux_output')(hidden2)\n",
    "model = keras.Model(inputs=[input_A,input_B],outputs=[output,aux_output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.8726 - main_output_loss: 0.8161 - aux_output_loss: 1.3807 - val_loss: 0.6126 - val_main_output_loss: 0.6032 - val_aux_output_loss: 0.6973\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5878 - main_output_loss: 0.5804 - aux_output_loss: 0.6550 - val_loss: 0.5756 - val_main_output_loss: 0.5697 - val_aux_output_loss: 0.6285\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5660 - main_output_loss: 0.5610 - aux_output_loss: 0.6107 - val_loss: 0.5627 - val_main_output_loss: 0.5578 - val_aux_output_loss: 0.6064\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5530 - main_output_loss: 0.5490 - aux_output_loss: 0.5892 - val_loss: 0.5482 - val_main_output_loss: 0.5442 - val_aux_output_loss: 0.5842\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5445 - main_output_loss: 0.5412 - aux_output_loss: 0.5737 - val_loss: 0.5454 - val_main_output_loss: 0.5423 - val_aux_output_loss: 0.5732\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5350 - main_output_loss: 0.5321 - aux_output_loss: 0.5607 - val_loss: 0.5443 - val_main_output_loss: 0.5421 - val_aux_output_loss: 0.5638\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5281 - main_output_loss: 0.5256 - aux_output_loss: 0.5511 - val_loss: 0.5349 - val_main_output_loss: 0.5327 - val_aux_output_loss: 0.5553\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5239 - main_output_loss: 0.5217 - aux_output_loss: 0.5435 - val_loss: 0.5264 - val_main_output_loss: 0.5240 - val_aux_output_loss: 0.5482\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5178 - main_output_loss: 0.5157 - aux_output_loss: 0.5368 - val_loss: 0.5264 - val_main_output_loss: 0.5244 - val_aux_output_loss: 0.5447\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5125 - main_output_loss: 0.5105 - aux_output_loss: 0.5307 - val_loss: 0.5262 - val_main_output_loss: 0.5247 - val_aux_output_loss: 0.5396\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5092 - main_output_loss: 0.5074 - aux_output_loss: 0.5252 - val_loss: 0.5191 - val_main_output_loss: 0.5174 - val_aux_output_loss: 0.5336\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5037 - main_output_loss: 0.5019 - aux_output_loss: 0.5203 - val_loss: 0.5130 - val_main_output_loss: 0.5110 - val_aux_output_loss: 0.5304\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5001 - main_output_loss: 0.4982 - aux_output_loss: 0.5170 - val_loss: 0.5087 - val_main_output_loss: 0.5068 - val_aux_output_loss: 0.5255\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4986 - main_output_loss: 0.4969 - aux_output_loss: 0.5131 - val_loss: 0.5076 - val_main_output_loss: 0.5058 - val_aux_output_loss: 0.5234\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4966 - main_output_loss: 0.4951 - aux_output_loss: 0.5107 - val_loss: 0.5128 - val_main_output_loss: 0.5116 - val_aux_output_loss: 0.5237\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4962 - main_output_loss: 0.4947 - aux_output_loss: 0.5095 - val_loss: 0.5122 - val_main_output_loss: 0.5111 - val_aux_output_loss: 0.5219\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4929 - main_output_loss: 0.4914 - aux_output_loss: 0.5066 - val_loss: 0.5054 - val_main_output_loss: 0.5039 - val_aux_output_loss: 0.5190\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4898 - main_output_loss: 0.4883 - aux_output_loss: 0.5034 - val_loss: 0.5051 - val_main_output_loss: 0.5037 - val_aux_output_loss: 0.5174\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4899 - main_output_loss: 0.4884 - aux_output_loss: 0.5028 - val_loss: 0.5055 - val_main_output_loss: 0.5041 - val_aux_output_loss: 0.5182\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4891 - main_output_loss: 0.4877 - aux_output_loss: 0.5014 - val_loss: 0.5075 - val_main_output_loss: 0.5065 - val_aux_output_loss: 0.5165\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 0.5869 - main_output_loss: 0.5875 - aux_output_loss: 0.5816\n"
     ]
    }
   ],
   "source": [
    "# 각 loss에 가중치를 줘야함 , 안주면 1:1 ,규제용으로 보조 출력을 이용한다는 가정하에 aux_output은 가중치를 0.1만 준다.\n",
    "model.compile(loss=['mse','mse'],optimizer='sgd',loss_weights=[0.9,0.1])\n",
    "\n",
    "# 출력이 두개이므로 y값도 두개를 전달해야 함.\n",
    "history = history = model.fit([X_train_A,X_train_B],[y_train,y_train],epochs=20,validation_data=((X_valid_A,X_valid_B),[y_valid,y_valid]))\n",
    "mse_test = model.evaluate((X_test_A,X_test_B),[y_test,y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.586949348449707, 0.587539553642273, 0.5816385746002197]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total_loss , main_loss , aux_loss\n",
    "mse_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파이토치와 같이 subclassing으로 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "input_A = keras.layers.Input(shape=[5],name='wide_input')\n",
    "input_B = keras.layers.Input(shape=[6],name='deep_input')\n",
    "hidden1 = keras.layers.Dense(30,activation='relu')(input_A)\n",
    "hidden2 = keras.layers.Dense(30,activation='relu')(hidden1)\n",
    "concat = keras.layers.concatenate([input_A,hidden2])\n",
    "output = keras.layers.Dense(1,name='main_output')(concat)\n",
    "aux_output = keras.layers.Dense(1,name='aux_output')(hidden2)\n",
    "model = keras.Model(inputs=[input_A,input_B],outputs=[output,aux_output])\n",
    "'''\n",
    "\n",
    "class WideDeepModel(keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(30,activation='relu')\n",
    "        self.hidden2 = keras.layers.Dense(30,activation='relu')\n",
    "        self.main_output = keras.layers.Dense(1)\n",
    "        self.aux_output = keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self,inputs):          # 파이토치는 def forward로 선언하는 것이 차이\n",
    "        input_A , input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input_A,hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output , aux_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 2.6520 - output_1_loss: 2.5336 - output_2_loss: 3.7181 - val_loss: 1.2616 - val_output_1_loss: 1.1138 - val_output_2_loss: 2.5919\n",
      "Epoch 2/5\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.1078 - output_1_loss: 0.9873 - output_2_loss: 2.1917 - val_loss: 0.9182 - val_output_1_loss: 0.8183 - val_output_2_loss: 1.8168\n",
      "Epoch 3/5\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.8944 - output_1_loss: 0.8021 - output_2_loss: 1.7246 - val_loss: 0.8027 - val_output_1_loss: 0.7218 - val_output_2_loss: 1.5310\n",
      "Epoch 4/5\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.8082 - output_1_loss: 0.7258 - output_2_loss: 1.5495 - val_loss: 0.7417 - val_output_1_loss: 0.6672 - val_output_2_loss: 1.4120\n",
      "Epoch 5/5\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7585 - output_1_loss: 0.6795 - output_2_loss: 1.4698 - val_loss: 0.7027 - val_output_1_loss: 0.6306 - val_output_2_loss: 1.3522\n",
      "162/162 [==============================] - 0s 801us/step - loss: 0.7546 - output_1_loss: 0.6735 - output_2_loss: 1.4843\n"
     ]
    }
   ],
   "source": [
    "model = WideDeepModel()\n",
    "model.compile(loss=['mse','mse'],optimizer=keras.optimizers.SGD(learning_rate=1e-3),loss_weights=[0.9,0.1])\n",
    "history = model.fit([X_train_A,X_train_B],[y_train,y_train],epochs=5,validation_data=((X_valid_A,X_valid_B),[y_valid,y_valid]))\n",
    "mse_test = model.evaluate((X_test_A,X_test_B),[y_test,y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7545643448829651, 0.6734825372695923, 1.4842987060546875]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF모델 저장 & 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "응 컴파일러 선언안했어\n"
     ]
    }
   ],
   "source": [
    "# subclassing 방법은 weight만 저장 가능, compiler같은 setting은 저장안됨\n",
    "model.save_weights('model_weights.h5')\n",
    "model1 = WideDeepModel()\n",
    "model1.build(input_shape=[(None, 5), (None, 6)])  # 가중치 생성 , 모델 객체를 만든 후 call 혹은 build 메소드를 호출해야 가중치가 생김\n",
    "model1.load_weights('model_weights.h5')          # HDF5 파일에서 가중치 로드\n",
    "try:\n",
    "    print(model1.optimizer.learning_rate)\n",
    "except:\n",
    "    print('응 컴파일러 선언안했어')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 저장 Sequential이나 model = keras.Model(inputs=[input_A,input_B],outputs=[output,aux_output])방식만 됨\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])    \n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "model.save('my_keras_model.h5')\n",
    "model = keras.models.load_model(\"my_keras_model.h5\")\n",
    "model.optimizer.learning_rate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "콜백(checkpoint)\n",
    "- 텐서플로는 collback함수를 지원한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "316/363 [=========================>....] - ETA: 0s - loss: 2.1121WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "363/363 [==============================] - 0s 655us/step - loss: 1.9672\n",
      "Epoch 2/10\n",
      "308/363 [========================>.....] - ETA: 0s - loss: 0.8891WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "363/363 [==============================] - 0s 671us/step - loss: 0.8763\n",
      "Epoch 3/10\n",
      "308/363 [========================>.....] - ETA: 0s - loss: 0.7999WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "363/363 [==============================] - 0s 666us/step - loss: 0.7937\n",
      "Epoch 4/10\n",
      "301/363 [=======================>......] - ETA: 0s - loss: 0.7461WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "363/363 [==============================] - 0s 674us/step - loss: 0.7426\n",
      "Epoch 5/10\n",
      "304/363 [========================>.....] - ETA: 0s - loss: 0.7042WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "363/363 [==============================] - 0s 669us/step - loss: 0.7038\n",
      "Epoch 6/10\n",
      "314/363 [========================>.....] - ETA: 0s - loss: 0.6686WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "363/363 [==============================] - 0s 652us/step - loss: 0.6727\n",
      "Epoch 7/10\n",
      "306/363 [========================>.....] - ETA: 0s - loss: 0.6603WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "363/363 [==============================] - 0s 669us/step - loss: 0.6467\n",
      "Epoch 8/10\n",
      "307/363 [========================>.....] - ETA: 0s - loss: 0.6279WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "363/363 [==============================] - 0s 671us/step - loss: 0.6242\n",
      "Epoch 9/10\n",
      "305/363 [========================>.....] - ETA: 0s - loss: 0.6107WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "363/363 [==============================] - 0s 669us/step - loss: 0.6047\n",
      "Epoch 10/10\n",
      "309/363 [========================>.....] - ETA: 0s - loss: 0.5834WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "363/363 [==============================] - 0s 663us/step - loss: 0.5877\n"
     ]
    }
   ],
   "source": [
    "# 최상의 모델만 저장\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30,activation='relu',input_shape=X_train.shape[1:]),     # input.shape는 (데이터 개수 , ~ , ~)일 것이므로\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\",save_best_only=True)    # save_best_only옵션은 최상의 valid set에 대해서만 저장함 , 과대적합 걱정x\n",
    "history = model.fit(X_train,y_train,epochs=10,callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 2.1653 - val_loss: 1.0359\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.8554 - val_loss: 0.7413\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6845 - val_loss: 0.6690\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6347 - val_loss: 0.6319\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6015 - val_loss: 0.6040\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5754 - val_loss: 0.5807\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5535 - val_loss: 0.5613\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5353 - val_loss: 0.5456\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5194 - val_loss: 0.5323\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5058 - val_loss: 0.5203\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4937 - val_loss: 0.5101\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4837 - val_loss: 0.5015\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4744 - val_loss: 0.4941\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4667 - val_loss: 0.4872\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4592 - val_loss: 0.4806\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4530 - val_loss: 0.4754\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4471 - val_loss: 0.4703\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4419 - val_loss: 0.4659\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4372 - val_loss: 0.4623\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4330 - val_loss: 0.4592\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4295 - val_loss: 0.4559\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4261 - val_loss: 0.4534\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4230 - val_loss: 0.4511\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4202 - val_loss: 0.4490\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4178 - val_loss: 0.4474\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4152 - val_loss: 0.4457\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4131 - val_loss: 0.4444\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4111 - val_loss: 0.4426\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4092 - val_loss: 0.4421\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4074 - val_loss: 0.4405\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4058 - val_loss: 0.4390\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4042 - val_loss: 0.4385\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4026 - val_loss: 0.4373\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4013 - val_loss: 0.4360\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3999 - val_loss: 0.4357\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3986 - val_loss: 0.4353\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3975 - val_loss: 0.4344\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3963 - val_loss: 0.4335\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3951 - val_loss: 0.4335\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3943 - val_loss: 0.4329\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3932 - val_loss: 0.4324\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3921 - val_loss: 0.4313\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3911 - val_loss: 0.4303\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3903 - val_loss: 0.4299\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3894 - val_loss: 0.4298\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3886 - val_loss: 0.4299\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3877 - val_loss: 0.4286\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3870 - val_loss: 0.4288\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3863 - val_loss: 0.4276\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3854 - val_loss: 0.4279\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3847 - val_loss: 0.4269\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3840 - val_loss: 0.4273\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3833 - val_loss: 0.4268\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3828 - val_loss: 0.4264\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3820 - val_loss: 0.4257\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3815 - val_loss: 0.4257\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3806 - val_loss: 0.4254\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3802 - val_loss: 0.4251\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3795 - val_loss: 0.4247\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3790 - val_loss: 0.4244\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3785 - val_loss: 0.4246\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3778 - val_loss: 0.4238\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3772 - val_loss: 0.4235\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3766 - val_loss: 0.4247\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3761 - val_loss: 0.4234\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3755 - val_loss: 0.4227\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3752 - val_loss: 0.4224\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3745 - val_loss: 0.4225\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3740 - val_loss: 0.4228\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3735 - val_loss: 0.4220\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3730 - val_loss: 0.4217\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3725 - val_loss: 0.4213\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3720 - val_loss: 0.4208\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3716 - val_loss: 0.4205\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3709 - val_loss: 0.4218\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3706 - val_loss: 0.4212\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3702 - val_loss: 0.4199\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3696 - val_loss: 0.4207\n",
      "Epoch 79/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3694 - val_loss: 0.4203\n",
      "Epoch 80/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3688 - val_loss: 0.4194\n",
      "Epoch 81/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3684 - val_loss: 0.4189\n",
      "Epoch 82/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3679 - val_loss: 0.4192\n",
      "Epoch 83/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3675 - val_loss: 0.4195\n",
      "Epoch 84/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3672 - val_loss: 0.4186\n",
      "Epoch 85/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3666 - val_loss: 0.4185\n",
      "Epoch 86/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3661 - val_loss: 0.4181\n",
      "Epoch 87/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3658 - val_loss: 0.4177\n",
      "Epoch 88/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3653 - val_loss: 0.4185\n",
      "Epoch 89/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3649 - val_loss: 0.4177\n",
      "Epoch 90/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3645 - val_loss: 0.4173\n",
      "Epoch 91/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3641 - val_loss: 0.4174\n",
      "Epoch 92/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3637 - val_loss: 0.4179\n",
      "Epoch 93/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3634 - val_loss: 0.4163\n",
      "Epoch 94/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3631 - val_loss: 0.4166\n",
      "Epoch 95/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3626 - val_loss: 0.4167\n",
      "Epoch 96/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3620 - val_loss: 0.4152\n",
      "Epoch 97/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3616 - val_loss: 0.4171\n",
      "Epoch 98/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3613 - val_loss: 0.4157\n",
      "Epoch 99/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3609 - val_loss: 0.4152\n",
      "Epoch 100/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3606 - val_loss: 0.4161\n",
      "162/162 [==============================] - 0s 734us/step - loss: 0.3925\n"
     ]
    }
   ],
   "source": [
    "# 학습의 진전이 없으면 조기 종료\n",
    "# callbacks여러개 사용 가능\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30,activation='relu',input_shape=X_train.shape[1:]),     # input.shape는 (데이터 개수 , ~ , ~)일 것이므로\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\",save_best_only=True)    # save_best_only옵션은 최상의 valid set에 대해서만 저장함 , 과대적합 걱정x\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,restore_best_weights=True)    # 일정 에포크(patience)동안 valid set에서 점수가 향상되지 않으면 조기 종료\n",
    "                                                                                            # restore_best_weights옵션은 훈련이 끝난 후 최상의 가중치를 복원함 \n",
    "                                                                                            # 즉,model = keras.models.load_model(\"my_keras_model.h5\") 같은 과정이 없어도 됨\n",
    "                                                                                            \n",
    "history = model.fit(X_train, y_train, epochs=100,                                           \n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb,early_stopping_cb])                            # checkpoint_cb을 안해주면 자동으로 저장을 안함\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "318/363 [=========================>....] - ETA: 0s - loss: 2.3553\n",
      "val/train: 0.46\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 2.1717 - val_loss: 0.9937\n",
      "Epoch 2/100\n",
      "313/363 [========================>.....] - ETA: 0s - loss: 0.8391\n",
      "val/train: 0.93\n",
      "363/363 [==============================] - 0s 905us/step - loss: 0.8320 - val_loss: 0.7749\n",
      "Epoch 3/100\n",
      "317/363 [=========================>....] - ETA: 0s - loss: 0.7222\n",
      "val/train: 1.01\n",
      "363/363 [==============================] - 0s 923us/step - loss: 0.7199 - val_loss: 0.7298\n",
      "Epoch 4/100\n",
      "312/363 [========================>.....] - ETA: 0s - loss: 0.7009\n",
      "val/train: 1.02\n",
      "363/363 [==============================] - 0s 935us/step - loss: 0.6865 - val_loss: 0.6987\n",
      "Epoch 5/100\n",
      "310/363 [========================>.....] - ETA: 0s - loss: 0.6612\n",
      "val/train: 1.02\n",
      "363/363 [==============================] - 0s 911us/step - loss: 0.6588 - val_loss: 0.6721\n",
      "Epoch 6/100\n",
      "315/363 [=========================>....] - ETA: 0s - loss: 0.6398\n",
      "val/train: 1.02\n",
      "363/363 [==============================] - 0s 928us/step - loss: 0.6335 - val_loss: 0.6483\n",
      "Epoch 7/100\n",
      "309/363 [========================>.....] - ETA: 0s - loss: 0.6045\n",
      "val/train: 1.02\n",
      "363/363 [==============================] - 0s 936us/step - loss: 0.6105 - val_loss: 0.6251\n",
      "Epoch 8/100\n",
      "303/363 [========================>.....] - ETA: 0s - loss: 0.5975\n",
      "val/train: 1.03\n",
      "363/363 [==============================] - 0s 951us/step - loss: 0.5901 - val_loss: 0.6053\n",
      "Epoch 9/100\n",
      "351/363 [============================>.] - ETA: 0s - loss: 0.5709\n",
      "val/train: 1.03\n",
      "363/363 [==============================] - 0s 992us/step - loss: 0.5708 - val_loss: 0.5860\n",
      "Epoch 10/100\n",
      "301/363 [=======================>......] - ETA: 0s - loss: 0.5548\n",
      "val/train: 1.03\n",
      "363/363 [==============================] - 0s 955us/step - loss: 0.5538 - val_loss: 0.5704\n",
      "Epoch 11/100\n",
      "299/363 [=======================>......] - ETA: 0s - loss: 0.5401\n",
      "val/train: 1.03\n",
      "363/363 [==============================] - 0s 964us/step - loss: 0.5380 - val_loss: 0.5548\n",
      "Epoch 12/100\n",
      "302/363 [=======================>......] - ETA: 0s - loss: 0.5335\n",
      "val/train: 1.03\n",
      "363/363 [==============================] - 0s 951us/step - loss: 0.5242 - val_loss: 0.5411\n",
      "Epoch 13/100\n",
      "306/363 [========================>.....] - ETA: 0s - loss: 0.5077\n",
      "val/train: 1.04\n",
      "363/363 [==============================] - 0s 935us/step - loss: 0.5113 - val_loss: 0.5300\n",
      "Epoch 14/100\n",
      "305/363 [========================>.....] - ETA: 0s - loss: 0.5000\n",
      "val/train: 1.04\n",
      "363/363 [==============================] - 0s 939us/step - loss: 0.5000 - val_loss: 0.5201\n",
      "Epoch 15/100\n",
      "312/363 [========================>.....] - ETA: 0s - loss: 0.4873\n",
      "val/train: 1.04\n",
      "363/363 [==============================] - 0s 935us/step - loss: 0.4897 - val_loss: 0.5110\n",
      "Epoch 16/100\n",
      "312/363 [========================>.....] - ETA: 0s - loss: 0.4757\n",
      "val/train: 1.05\n",
      "363/363 [==============================] - 0s 933us/step - loss: 0.4806 - val_loss: 0.5025\n",
      "Epoch 17/100\n",
      "308/363 [========================>.....] - ETA: 0s - loss: 0.4797\n",
      "val/train: 1.05\n",
      "363/363 [==============================] - 0s 922us/step - loss: 0.4725 - val_loss: 0.4959\n",
      "Epoch 18/100\n",
      "310/363 [========================>.....] - ETA: 0s - loss: 0.4722\n",
      "val/train: 1.05\n",
      "363/363 [==============================] - 0s 919us/step - loss: 0.4652 - val_loss: 0.4894\n",
      "Epoch 19/100\n",
      "302/363 [=======================>......] - ETA: 0s - loss: 0.4670\n",
      "val/train: 1.05\n",
      "363/363 [==============================] - 0s 968us/step - loss: 0.4588 - val_loss: 0.4839\n",
      "Epoch 20/100\n",
      "303/363 [========================>.....] - ETA: 0s - loss: 0.4495\n",
      "val/train: 1.06\n",
      "363/363 [==============================] - 0s 949us/step - loss: 0.4527 - val_loss: 0.4799\n",
      "Epoch 21/100\n",
      "308/363 [========================>.....] - ETA: 0s - loss: 0.4408\n",
      "val/train: 1.06\n",
      "363/363 [==============================] - 0s 927us/step - loss: 0.4475 - val_loss: 0.4748\n",
      "Epoch 22/100\n",
      "303/363 [========================>.....] - ETA: 0s - loss: 0.4427\n",
      "val/train: 1.06\n",
      "363/363 [==============================] - 0s 964us/step - loss: 0.4426 - val_loss: 0.4701\n",
      "Epoch 23/100\n",
      "308/363 [========================>.....] - ETA: 0s - loss: 0.4395\n",
      "val/train: 1.06\n",
      "363/363 [==============================] - 0s 950us/step - loss: 0.4381 - val_loss: 0.4665\n",
      "Epoch 24/100\n",
      "303/363 [========================>.....] - ETA: 0s - loss: 0.4280\n",
      "val/train: 1.07\n",
      "363/363 [==============================] - 0s 953us/step - loss: 0.4337 - val_loss: 0.4634\n",
      "Epoch 25/100\n",
      "301/363 [=======================>......] - ETA: 0s - loss: 0.4329\n",
      "val/train: 1.07\n",
      "363/363 [==============================] - 0s 937us/step - loss: 0.4301 - val_loss: 0.4598\n",
      "Epoch 26/100\n",
      "300/363 [=======================>......] - ETA: 0s - loss: 0.4225\n",
      "val/train: 1.07\n",
      "363/363 [==============================] - 0s 972us/step - loss: 0.4264 - val_loss: 0.4572\n",
      "Epoch 27/100\n",
      "303/363 [========================>.....] - ETA: 0s - loss: 0.4220\n",
      "val/train: 1.07\n",
      "363/363 [==============================] - 0s 959us/step - loss: 0.4232 - val_loss: 0.4544\n",
      "Epoch 28/100\n",
      "303/363 [========================>.....] - ETA: 0s - loss: 0.4234\n",
      "val/train: 1.07\n",
      "363/363 [==============================] - 0s 952us/step - loss: 0.4201 - val_loss: 0.4514\n",
      "Epoch 29/100\n",
      "306/363 [========================>.....] - ETA: 0s - loss: 0.4162\n",
      "val/train: 1.08\n",
      "363/363 [==============================] - 0s 927us/step - loss: 0.4173 - val_loss: 0.4491\n",
      "Epoch 30/100\n",
      "304/363 [========================>.....] - ETA: 0s - loss: 0.4108\n",
      "val/train: 1.08\n",
      "363/363 [==============================] - 0s 928us/step - loss: 0.4146 - val_loss: 0.4475\n",
      "Epoch 31/100\n",
      "307/363 [========================>.....] - ETA: 0s - loss: 0.4130\n",
      "val/train: 1.08\n",
      "363/363 [==============================] - 0s 950us/step - loss: 0.4119 - val_loss: 0.4452\n",
      "Epoch 32/100\n",
      "302/363 [=======================>......] - ETA: 0s - loss: 0.4105\n",
      "val/train: 1.08\n",
      "363/363 [==============================] - 0s 967us/step - loss: 0.4097 - val_loss: 0.4431\n",
      "Epoch 33/100\n",
      "296/363 [=======================>......] - ETA: 0s - loss: 0.4059\n",
      "val/train: 1.09\n",
      "363/363 [==============================] - 0s 943us/step - loss: 0.4075 - val_loss: 0.4426\n",
      "Epoch 34/100\n",
      "301/363 [=======================>......] - ETA: 0s - loss: 0.4097\n",
      "val/train: 1.09\n",
      "363/363 [==============================] - 0s 984us/step - loss: 0.4052 - val_loss: 0.4398\n",
      "Epoch 35/100\n",
      "312/363 [========================>.....] - ETA: 0s - loss: 0.4031\n",
      "val/train: 1.09\n",
      "363/363 [==============================] - 0s 947us/step - loss: 0.4034 - val_loss: 0.4393\n",
      "Epoch 36/100\n",
      "312/363 [========================>.....] - ETA: 0s - loss: 0.4072\n",
      "val/train: 1.09\n",
      "363/363 [==============================] - 0s 932us/step - loss: 0.4015 - val_loss: 0.4375\n",
      "Epoch 37/100\n",
      "304/363 [========================>.....] - ETA: 0s - loss: 0.3998\n",
      "val/train: 1.09\n",
      "363/363 [==============================] - 0s 987us/step - loss: 0.3996 - val_loss: 0.4373\n",
      "Epoch 38/100\n",
      "304/363 [========================>.....] - ETA: 0s - loss: 0.4061\n",
      "val/train: 1.09\n",
      "363/363 [==============================] - 0s 951us/step - loss: 0.3979 - val_loss: 0.4351\n",
      "Epoch 39/100\n",
      "304/363 [========================>.....] - ETA: 0s - loss: 0.4012\n",
      "val/train: 1.10\n",
      "363/363 [==============================] - 0s 946us/step - loss: 0.3963 - val_loss: 0.4342\n",
      "Epoch 40/100\n",
      "315/363 [=========================>....] - ETA: 0s - loss: 0.3953\n",
      "val/train: 1.10\n",
      "363/363 [==============================] - 0s 925us/step - loss: 0.3949 - val_loss: 0.4328\n",
      "Epoch 41/100\n",
      "305/363 [========================>.....] - ETA: 0s - loss: 0.3916\n",
      "val/train: 1.10\n",
      "363/363 [==============================] - 0s 896us/step - loss: 0.3934 - val_loss: 0.4331\n",
      "Epoch 42/100\n",
      "300/363 [=======================>......] - ETA: 0s - loss: 0.3953\n",
      "val/train: 1.10\n",
      "363/363 [==============================] - 0s 931us/step - loss: 0.3921 - val_loss: 0.4314\n",
      "Epoch 43/100\n",
      "310/363 [========================>.....] - ETA: 0s - loss: 0.3954\n",
      "val/train: 1.10\n",
      "363/363 [==============================] - 0s 946us/step - loss: 0.3908 - val_loss: 0.4306\n",
      "Epoch 44/100\n",
      "311/363 [========================>.....] - ETA: 0s - loss: 0.3937\n",
      "val/train: 1.10\n",
      "363/363 [==============================] - 0s 914us/step - loss: 0.3896 - val_loss: 0.4294\n",
      "Epoch 45/100\n",
      "313/363 [========================>.....] - ETA: 0s - loss: 0.3794\n",
      "val/train: 1.10\n",
      "363/363 [==============================] - 0s 935us/step - loss: 0.3884 - val_loss: 0.4287\n",
      "Epoch 46/100\n",
      "297/363 [=======================>......] - ETA: 0s - loss: 0.3811\n",
      "val/train: 1.11\n",
      "363/363 [==============================] - 0s 953us/step - loss: 0.3874 - val_loss: 0.4283\n",
      "Epoch 47/100\n",
      "310/363 [========================>.....] - ETA: 0s - loss: 0.3885\n",
      "val/train: 1.11\n",
      "363/363 [==============================] - 0s 913us/step - loss: 0.3862 - val_loss: 0.4281\n",
      "Epoch 48/100\n",
      "311/363 [========================>.....] - ETA: 0s - loss: 0.3847\n",
      "val/train: 1.11\n",
      "363/363 [==============================] - 0s 933us/step - loss: 0.3852 - val_loss: 0.4268\n",
      "Epoch 49/100\n",
      "312/363 [========================>.....] - ETA: 0s - loss: 0.3827\n",
      "val/train: 1.11\n",
      "363/363 [==============================] - 0s 930us/step - loss: 0.3842 - val_loss: 0.4259\n",
      "Epoch 50/100\n",
      "313/363 [========================>.....] - ETA: 0s - loss: 0.3838\n",
      "val/train: 1.11\n",
      "363/363 [==============================] - 0s 871us/step - loss: 0.3832 - val_loss: 0.4269\n",
      "Epoch 51/100\n",
      "310/363 [========================>.....] - ETA: 0s - loss: 0.3864\n",
      "val/train: 1.11\n",
      "363/363 [==============================] - 0s 912us/step - loss: 0.3825 - val_loss: 0.4253\n",
      "Epoch 52/100\n",
      "306/363 [========================>.....] - ETA: 0s - loss: 0.3852\n",
      "val/train: 1.11\n",
      "363/363 [==============================] - 0s 940us/step - loss: 0.3816 - val_loss: 0.4247\n",
      "Epoch 53/100\n",
      "309/363 [========================>.....] - ETA: 0s - loss: 0.3763\n",
      "val/train: 1.12\n",
      "363/363 [==============================] - 0s 887us/step - loss: 0.3808 - val_loss: 0.4247\n",
      "Epoch 54/100\n",
      "309/363 [========================>.....] - ETA: 0s - loss: 0.3824\n",
      "val/train: 1.11\n",
      "363/363 [==============================] - 0s 946us/step - loss: 0.3799 - val_loss: 0.4233\n",
      "Epoch 55/100\n",
      "298/363 [=======================>......] - ETA: 0s - loss: 0.3793\n",
      "val/train: 1.12\n",
      "363/363 [==============================] - 0s 914us/step - loss: 0.3793 - val_loss: 0.4237\n",
      "Epoch 56/100\n",
      "300/363 [=======================>......] - ETA: 0s - loss: 0.3804\n",
      "val/train: 1.12\n",
      "363/363 [==============================] - 0s 960us/step - loss: 0.3784 - val_loss: 0.4232\n",
      "Epoch 57/100\n",
      "311/363 [========================>.....] - ETA: 0s - loss: 0.3706\n",
      "val/train: 1.12\n",
      "363/363 [==============================] - 0s 887us/step - loss: 0.3778 - val_loss: 0.4233\n",
      "Epoch 58/100\n",
      "306/363 [========================>.....] - ETA: 0s - loss: 0.3803\n",
      "val/train: 1.12\n",
      "363/363 [==============================] - 0s 953us/step - loss: 0.3771 - val_loss: 0.4230\n",
      "Epoch 59/100\n",
      "306/363 [========================>.....] - ETA: 0s - loss: 0.3769\n",
      "val/train: 1.12\n",
      "363/363 [==============================] - 0s 947us/step - loss: 0.3764 - val_loss: 0.4221\n",
      "Epoch 60/100\n",
      "310/363 [========================>.....] - ETA: 0s - loss: 0.3786\n",
      "val/train: 1.12\n",
      "363/363 [==============================] - 0s 939us/step - loss: 0.3758 - val_loss: 0.4217\n",
      "Epoch 61/100\n",
      "308/363 [========================>.....] - ETA: 0s - loss: 0.3725\n",
      "val/train: 1.12\n",
      "363/363 [==============================] - 0s 887us/step - loss: 0.3751 - val_loss: 0.4218\n",
      "Epoch 62/100\n",
      "310/363 [========================>.....] - ETA: 0s - loss: 0.3799\n",
      "val/train: 1.13\n",
      "363/363 [==============================] - 0s 934us/step - loss: 0.3744 - val_loss: 0.4212\n",
      "Epoch 63/100\n",
      "312/363 [========================>.....] - ETA: 0s - loss: 0.3774\n",
      "val/train: 1.13\n",
      "363/363 [==============================] - 0s 938us/step - loss: 0.3737 - val_loss: 0.4212\n",
      "Epoch 64/100\n",
      "306/363 [========================>.....] - ETA: 0s - loss: 0.3750\n",
      "val/train: 1.13\n",
      "363/363 [==============================] - 0s 940us/step - loss: 0.3731 - val_loss: 0.4200\n",
      "Epoch 65/100\n",
      "309/363 [========================>.....] - ETA: 0s - loss: 0.3729\n",
      "val/train: 1.13\n",
      "363/363 [==============================] - 0s 888us/step - loss: 0.3726 - val_loss: 0.4211\n",
      "Epoch 66/100\n",
      "309/363 [========================>.....] - ETA: 0s - loss: 0.3732\n",
      "val/train: 1.13\n",
      "363/363 [==============================] - 0s 884us/step - loss: 0.3718 - val_loss: 0.4215\n",
      "Epoch 67/100\n",
      "306/363 [========================>.....] - ETA: 0s - loss: 0.3748\n",
      "val/train: 1.13\n",
      "363/363 [==============================] - 0s 948us/step - loss: 0.3712 - val_loss: 0.4199\n",
      "Epoch 68/100\n",
      "310/363 [========================>.....] - ETA: 0s - loss: 0.3685\n",
      "val/train: 1.13\n",
      "363/363 [==============================] - 0s 938us/step - loss: 0.3706 - val_loss: 0.4187\n",
      "Epoch 69/100\n",
      "314/363 [========================>.....] - ETA: 0s - loss: 0.3685\n",
      "val/train: 1.13\n",
      "363/363 [==============================] - 0s 879us/step - loss: 0.3701 - val_loss: 0.4189\n",
      "Epoch 70/100\n",
      "303/363 [========================>.....] - ETA: 0s - loss: 0.3759\n",
      "val/train: 1.13\n",
      "363/363 [==============================] - 0s 934us/step - loss: 0.3694 - val_loss: 0.4185\n",
      "Epoch 71/100\n",
      "312/363 [========================>.....] - ETA: 0s - loss: 0.3708\n",
      "val/train: 1.14\n",
      "363/363 [==============================] - 0s 894us/step - loss: 0.3689 - val_loss: 0.4193\n",
      "Epoch 72/100\n",
      "314/363 [========================>.....] - ETA: 0s - loss: 0.3743\n",
      "val/train: 1.14\n",
      "363/363 [==============================] - 0s 930us/step - loss: 0.3685 - val_loss: 0.4182\n",
      "Epoch 73/100\n",
      "303/363 [========================>.....] - ETA: 0s - loss: 0.3610\n",
      "val/train: 1.14\n",
      "363/363 [==============================] - 0s 936us/step - loss: 0.3679 - val_loss: 0.4179\n",
      "Epoch 74/100\n",
      "311/363 [========================>.....] - ETA: 0s - loss: 0.3682\n",
      "val/train: 1.13\n",
      "363/363 [==============================] - 0s 936us/step - loss: 0.3673 - val_loss: 0.4169\n",
      "Epoch 75/100\n",
      "313/363 [========================>.....] - ETA: 0s - loss: 0.3661\n",
      "val/train: 1.14\n",
      "363/363 [==============================] - 0s 875us/step - loss: 0.3669 - val_loss: 0.4177\n",
      "Epoch 76/100\n",
      "311/363 [========================>.....] - ETA: 0s - loss: 0.3653\n",
      "val/train: 1.14\n",
      "363/363 [==============================] - 0s 882us/step - loss: 0.3664 - val_loss: 0.4170\n",
      "Epoch 77/100\n",
      "310/363 [========================>.....] - ETA: 0s - loss: 0.3677\n",
      "val/train: 1.14\n",
      "363/363 [==============================] - 0s 918us/step - loss: 0.3660 - val_loss: 0.4165\n",
      "Epoch 78/100\n",
      "362/363 [============================>.] - ETA: 0s - loss: 0.3655\n",
      "val/train: 1.14\n",
      "363/363 [==============================] - 0s 984us/step - loss: 0.3653 - val_loss: 0.4162\n",
      "Epoch 79/100\n",
      "311/363 [========================>.....] - ETA: 0s - loss: 0.3607\n",
      "val/train: 1.14\n",
      "363/363 [==============================] - 0s 884us/step - loss: 0.3650 - val_loss: 0.4176\n",
      "Epoch 80/100\n",
      "302/363 [=======================>......] - ETA: 0s - loss: 0.3638\n",
      "val/train: 1.14\n",
      "363/363 [==============================] - 0s 974us/step - loss: 0.3644 - val_loss: 0.4160\n",
      "Epoch 81/100\n",
      "302/363 [=======================>......] - ETA: 0s - loss: 0.3604\n",
      "val/train: 1.14\n",
      "363/363 [==============================] - 0s 951us/step - loss: 0.3638 - val_loss: 0.4152\n",
      "Epoch 82/100\n",
      "304/363 [========================>.....] - ETA: 0s - loss: 0.3575\n",
      "val/train: 1.15\n",
      "363/363 [==============================] - 0s 905us/step - loss: 0.3634 - val_loss: 0.4167\n",
      "Epoch 83/100\n",
      "314/363 [========================>.....] - ETA: 0s - loss: 0.3635\n",
      "val/train: 1.14\n",
      "363/363 [==============================] - 0s 882us/step - loss: 0.3629 - val_loss: 0.4153\n",
      "Epoch 84/100\n",
      "309/363 [========================>.....] - ETA: 0s - loss: 0.3652\n",
      "val/train: 1.14\n",
      "363/363 [==============================] - 0s 935us/step - loss: 0.3625 - val_loss: 0.4147\n",
      "Epoch 85/100\n",
      "299/363 [=======================>......] - ETA: 0s - loss: 0.3656\n",
      "val/train: 1.14\n",
      "363/363 [==============================] - 0s 930us/step - loss: 0.3623 - val_loss: 0.4141\n",
      "Epoch 86/100\n",
      "312/363 [========================>.....] - ETA: 0s - loss: 0.3606\n",
      "val/train: 1.14\n",
      "363/363 [==============================] - 0s 880us/step - loss: 0.3619 - val_loss: 0.4142\n",
      "Epoch 87/100\n",
      "309/363 [========================>.....] - ETA: 0s - loss: 0.3612\n",
      "val/train: 1.15\n",
      "363/363 [==============================] - 0s 887us/step - loss: 0.3613 - val_loss: 0.4148\n",
      "Epoch 88/100\n",
      "310/363 [========================>.....] - ETA: 0s - loss: 0.3571\n",
      "val/train: 1.15\n",
      "363/363 [==============================] - 0s 945us/step - loss: 0.3613 - val_loss: 0.4138\n",
      "Epoch 89/100\n",
      "306/363 [========================>.....] - ETA: 0s - loss: 0.3583\n",
      "val/train: 1.15\n",
      "363/363 [==============================] - 0s 924us/step - loss: 0.3605 - val_loss: 0.4137\n",
      "Epoch 90/100\n",
      "310/363 [========================>.....] - ETA: 0s - loss: 0.3585\n",
      "val/train: 1.15\n",
      "363/363 [==============================] - 0s 938us/step - loss: 0.3601 - val_loss: 0.4131\n",
      "Epoch 91/100\n",
      "311/363 [========================>.....] - ETA: 0s - loss: 0.3556\n",
      "val/train: 1.15\n",
      "363/363 [==============================] - 0s 879us/step - loss: 0.3596 - val_loss: 0.4136\n",
      "Epoch 92/100\n",
      "311/363 [========================>.....] - ETA: 0s - loss: 0.3643\n",
      "val/train: 1.15\n",
      "363/363 [==============================] - 0s 925us/step - loss: 0.3593 - val_loss: 0.4128\n",
      "Epoch 93/100\n",
      "305/363 [========================>.....] - ETA: 0s - loss: 0.3606\n",
      "val/train: 1.15\n",
      "363/363 [==============================] - 0s 895us/step - loss: 0.3585 - val_loss: 0.4134\n",
      "Epoch 94/100\n",
      "312/363 [========================>.....] - ETA: 0s - loss: 0.3573\n",
      "val/train: 1.15\n",
      "363/363 [==============================] - 0s 930us/step - loss: 0.3589 - val_loss: 0.4122\n",
      "Epoch 95/100\n",
      "309/363 [========================>.....] - ETA: 0s - loss: 0.3586\n",
      "val/train: 1.15\n",
      "363/363 [==============================] - 0s 890us/step - loss: 0.3581 - val_loss: 0.4131\n",
      "Epoch 96/100\n",
      "310/363 [========================>.....] - ETA: 0s - loss: 0.3638\n",
      "val/train: 1.15\n",
      "363/363 [==============================] - 0s 887us/step - loss: 0.3578 - val_loss: 0.4131\n",
      "Epoch 97/100\n",
      "310/363 [========================>.....] - ETA: 0s - loss: 0.3613\n",
      "val/train: 1.15\n",
      "363/363 [==============================] - 0s 933us/step - loss: 0.3574 - val_loss: 0.4112\n",
      "Epoch 98/100\n",
      "309/363 [========================>.....] - ETA: 0s - loss: 0.3531\n",
      "val/train: 1.16\n",
      "363/363 [==============================] - 0s 886us/step - loss: 0.3570 - val_loss: 0.4124\n",
      "Epoch 99/100\n",
      "298/363 [=======================>......] - ETA: 0s - loss: 0.3610\n",
      "val/train: 1.15\n",
      "363/363 [==============================] - 0s 954us/step - loss: 0.3568 - val_loss: 0.4105\n",
      "Epoch 100/100\n",
      "307/363 [========================>.....] - ETA: 0s - loss: 0.3540\n",
      "val/train: 1.16\n",
      "363/363 [==============================] - 0s 898us/step - loss: 0.3564 - val_loss: 0.4122\n",
      "162/162 [==============================] - 0s 643us/step - loss: 0.3913\n"
     ]
    }
   ],
   "source": [
    "# 사용자 정의 callback함수\n",
    "# 과대적합 방지 : 검증손실/훈련손실 -> 해당 수치가 높아지면 과적합임\n",
    "\n",
    "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print(\"\\nval/train: {:.2f}\".format(logs[\"val_loss\"] / logs[\"loss\"]))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30,activation='relu',input_shape=X_train.shape[1:]),     # input.shape는 (데이터 개수 , ~ , ~)일 것이므로\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\",save_best_only=True)    # save_best_only옵션은 최상의 valid set에 대해서만 저장함 , 과대적합 걱정x\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,restore_best_weights=True)    # 일정 에포크(patience)동안 valid set에서 점수가 향상되지 않으면 조기 종료\n",
    "                                                                                            # restore_best_weights옵션은 훈련이 끝난 후 최상의 가중치를 복원함 \n",
    "                                                                                            # 즉,model = keras.models.load_model(\"my_keras_model.h5\") 같은 과정이 없어도 됨\n",
    "val_train_ratio_cb = PrintValTrainRatioCallback()\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100,                                           \n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb,early_stopping_cb,val_train_ratio_cb])                            # checkpoint_cb을 안해주면 자동으로 저장을 안함\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서보드 활용하기\n",
    "- 텐서보드는 루트 디렉토리 밑에 서브 디렉토리들이 존재한다.\n",
    "    - 서브 디렉토리에 내가 원하는 프로그램의 결과를 저장할 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\my_logs\\run_2023_04_16-15_30_22\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "root_logdir = os.path.join(os.curdir,\"my_logs\")\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime('run_%Y_%m_%d-%H_%M_%S')\n",
    "    return os.path.join(root_logdir,run_id)\n",
    "run_logdir = get_run_logdir()\n",
    "print(run_logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 3.0835 - val_loss: 0.8493\n",
      "Epoch 2/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7992 - val_loss: 0.5915\n",
      "Epoch 3/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6378 - val_loss: 0.5580\n",
      "Epoch 4/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6067 - val_loss: 0.5346\n",
      "Epoch 5/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5840 - val_loss: 0.5143\n",
      "Epoch 6/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5661 - val_loss: 0.4980\n",
      "Epoch 7/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5505 - val_loss: 0.4830\n",
      "Epoch 8/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5370 - val_loss: 0.4709\n",
      "Epoch 9/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5258 - val_loss: 0.4610\n",
      "Epoch 10/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5160 - val_loss: 0.4536\n",
      "Epoch 11/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5072 - val_loss: 0.4445\n",
      "Epoch 12/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4995 - val_loss: 0.4430\n",
      "Epoch 13/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4933 - val_loss: 0.4392\n",
      "Epoch 14/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4872 - val_loss: 0.4350\n",
      "Epoch 15/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4825 - val_loss: 0.4334\n",
      "Epoch 16/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4775 - val_loss: 0.4314\n",
      "Epoch 17/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4737 - val_loss: 0.4327\n",
      "Epoch 18/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4696 - val_loss: 0.4339\n",
      "Epoch 19/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4663 - val_loss: 0.4359\n",
      "Epoch 20/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4630 - val_loss: 0.4376\n",
      "Epoch 21/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4601 - val_loss: 0.4387\n",
      "Epoch 22/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4573 - val_loss: 0.4434\n",
      "Epoch 23/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4544 - val_loss: 0.4443\n",
      "Epoch 24/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4524 - val_loss: 0.4504\n",
      "Epoch 25/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4500 - val_loss: 0.4588\n",
      "Epoch 26/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4480 - val_loss: 0.4567\n",
      "Epoch 27/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4458 - val_loss: 0.4656\n",
      "Epoch 28/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4440 - val_loss: 0.4680\n",
      "Epoch 29/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4421 - val_loss: 0.4738\n",
      "Epoch 30/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4401 - val_loss: 0.4793\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30,activation='relu',input_shape=X_train.shape[1:]),     # input.shape는 (데이터 개수 , ~ , ~)일 것이므로\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)                        # 텐서보드 callback함수에 결국 디렉토리를 넘겨주는 것임\n",
    "history = model.fit(X_train, y_train, epochs=30,                                           \n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[tensorboard_cb])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서보드는 CMD에서 가상환경 실행 후 tensorboard --logdir=./my_logs --port=6006\n",
    "# 셸이 tensorboard스크립트를 찾지 못하면 PATH환경변수에 스크립트 설치 경로를 포함시켜야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-68c7c45e9e6b59f3\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-68c7c45e9e6b59f3\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 주피터 노트북으로 텐서보드 실행하기\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./my_logs --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow 그래프 삭제 및 메모리 해제\n",
    "# 여러번 모델을 돌렸으므로\n",
    "\n",
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하이퍼 파라미터 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_38784\\2818905346.py:13: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 1s 1ms/step - loss: 1.2174 - val_loss: 0.8109\n",
      "Epoch 2/3\n",
      "363/363 [==============================] - 0s 883us/step - loss: 0.8713 - val_loss: 0.6589\n",
      "Epoch 3/3\n",
      "363/363 [==============================] - 0s 874us/step - loss: 0.7063 - val_loss: 0.5145\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1717ad1b1c0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "\n",
    "# 주택가격 예측 모델이므로 Regressor\n",
    "# 쉽게 생각해서 model을 wrapper로 감싸서 model의 기능과 sklearn의 모델기능을 모두 사용할 수 있다.\n",
    "\n",
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n",
    "keras_reg.fit(X_train, y_train, epochs=3,\n",
    "              validation_data=(X_valid, y_valid),\n",
    "              callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 1.1228 - val_loss: 0.6713\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6426 - val_loss: 0.5442\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5446 - val_loss: 0.4805\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4865 - val_loss: 0.4617\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4538 - val_loss: 0.4665\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4326 - val_loss: 0.4949\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4188 - val_loss: 0.5144\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4085 - val_loss: 0.5277\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3988 - val_loss: 0.5665\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3912 - val_loss: 0.6293\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3848 - val_loss: 0.6424\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3794 - val_loss: 0.6642\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3737 - val_loss: 0.6667\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3710 - val_loss: 0.7866\n",
      "121/121 [==============================] - 0s 684us/step - loss: 0.3923\n",
      "[CV] END learning_rate=0.005203552368310722, n_hidden=3, n_neurons=38; total time=   4.1s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 1.0312 - val_loss: 0.6450\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5600 - val_loss: 0.4610\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4665 - val_loss: 0.5041\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4282 - val_loss: 0.5346\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4093 - val_loss: 0.6757\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4058 - val_loss: 0.6658\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3870 - val_loss: 0.7306\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3802 - val_loss: 0.7466\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3755 - val_loss: 0.8158\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3688 - val_loss: 0.8210\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3705 - val_loss: 0.8620\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3624 - val_loss: 0.8236\n",
      "121/121 [==============================] - 0s 702us/step - loss: 0.3716\n",
      "[CV] END learning_rate=0.005203552368310722, n_hidden=3, n_neurons=38; total time=   3.6s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 1.1609 - val_loss: 0.5607\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5489 - val_loss: 0.4787\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4946 - val_loss: 0.4340\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4636 - val_loss: 0.4260\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4401 - val_loss: 0.4553\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4256 - val_loss: 0.4603\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4135 - val_loss: 0.4819\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4024 - val_loss: 0.5267\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3943 - val_loss: 0.5592\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3855 - val_loss: 0.6285\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3809 - val_loss: 0.6570\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3758 - val_loss: 0.6583\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3682 - val_loss: 0.7438\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3666 - val_loss: 0.8455\n",
      "121/121 [==============================] - 0s 683us/step - loss: 0.3604\n",
      "[CV] END learning_rate=0.005203552368310722, n_hidden=3, n_neurons=38; total time=   4.2s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 0.8889 - val_loss: 0.8153\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9095 - val_loss: 0.5113\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4910 - val_loss: 0.5402\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4404 - val_loss: 0.5497\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4208 - val_loss: 0.5840\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4101 - val_loss: 0.6335\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4022 - val_loss: 0.6735\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3969 - val_loss: 0.6901\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3916 - val_loss: 0.7411\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3876 - val_loss: 0.7724\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3844 - val_loss: 0.8045\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3810 - val_loss: 0.8582\n",
      "121/121 [==============================] - 0s 693us/step - loss: 0.4001\n",
      "[CV] END learning_rate=0.008232368671389274, n_hidden=1, n_neurons=60; total time=   3.4s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 1.0993 - val_loss: 1.0076\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6604 - val_loss: 0.5242\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5197 - val_loss: 0.4861\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5911 - val_loss: 0.5263\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5051 - val_loss: 0.4908\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4355 - val_loss: 0.5163\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4235 - val_loss: 0.5451\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4149 - val_loss: 0.5829\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4105 - val_loss: 0.6177\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4076 - val_loss: 0.6674\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4022 - val_loss: 0.6770\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3969 - val_loss: 0.7091\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3926 - val_loss: 0.7364\n",
      "121/121 [==============================] - 0s 713us/step - loss: 0.4044\n",
      "[CV] END learning_rate=0.008232368671389274, n_hidden=1, n_neurons=60; total time=   3.6s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 1.0173 - val_loss: 0.5788\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5540 - val_loss: 0.4857\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5030 - val_loss: 0.4472\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4812 - val_loss: 0.4494\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4529 - val_loss: 0.5027\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4454 - val_loss: 0.4901\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4336 - val_loss: 0.5110\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4280 - val_loss: 0.5479\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4198 - val_loss: 0.5800\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4096 - val_loss: 0.6320\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4365 - val_loss: 0.6495\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4210 - val_loss: 0.6575\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3997 - val_loss: 0.7081\n",
      "121/121 [==============================] - 0s 666us/step - loss: 0.3872\n",
      "[CV] END learning_rate=0.008232368671389274, n_hidden=1, n_neurons=60; total time=   3.6s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 4.2021 - val_loss: 2.4562\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.9420 - val_loss: 1.3870\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2584 - val_loss: 1.0151\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9741 - val_loss: 0.8457\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8440 - val_loss: 0.7640\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7780 - val_loss: 0.7192\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7408 - val_loss: 0.6932\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7155 - val_loss: 0.6719\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6966 - val_loss: 0.6566\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6808 - val_loss: 0.6416\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6676 - val_loss: 0.6287\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6554 - val_loss: 0.6168\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6442 - val_loss: 0.6064\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6336 - val_loss: 0.5959\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6238 - val_loss: 0.5862\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6146 - val_loss: 0.5758\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6060 - val_loss: 0.5668\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5978 - val_loss: 0.5585\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5903 - val_loss: 0.5511\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5828 - val_loss: 0.5441\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5759 - val_loss: 0.5363\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5694 - val_loss: 0.5291\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5630 - val_loss: 0.5224\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5572 - val_loss: 0.5173\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5513 - val_loss: 0.5106\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5461 - val_loss: 0.5055\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5407 - val_loss: 0.4996\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5362 - val_loss: 0.4951\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5314 - val_loss: 0.4905\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5268 - val_loss: 0.4858\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5227 - val_loss: 0.4816\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5186 - val_loss: 0.4781\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5148 - val_loss: 0.4741\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5110 - val_loss: 0.4703\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5076 - val_loss: 0.4674\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5041 - val_loss: 0.4646\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5010 - val_loss: 0.4617\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4980 - val_loss: 0.4589\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4950 - val_loss: 0.4560\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4922 - val_loss: 0.4533\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4894 - val_loss: 0.4510\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4867 - val_loss: 0.4486\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4842 - val_loss: 0.4463\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4819 - val_loss: 0.4442\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4795 - val_loss: 0.4431\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4771 - val_loss: 0.4403\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4748 - val_loss: 0.4401\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4729 - val_loss: 0.4378\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4708 - val_loss: 0.4357\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4687 - val_loss: 0.4353\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4668 - val_loss: 0.4334\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4649 - val_loss: 0.4325\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4629 - val_loss: 0.4307\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4613 - val_loss: 0.4298\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4594 - val_loss: 0.4291\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4577 - val_loss: 0.4280\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4559 - val_loss: 0.4269\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4543 - val_loss: 0.4264\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4527 - val_loss: 0.4257\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4513 - val_loss: 0.4248\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4497 - val_loss: 0.4242\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4482 - val_loss: 0.4246\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4468 - val_loss: 0.4241\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4454 - val_loss: 0.4235\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4440 - val_loss: 0.4229\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4427 - val_loss: 0.4232\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4414 - val_loss: 0.4228\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4400 - val_loss: 0.4221\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4388 - val_loss: 0.4226\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4375 - val_loss: 0.4227\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4363 - val_loss: 0.4233\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4352 - val_loss: 0.4230\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4338 - val_loss: 0.4223\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4328 - val_loss: 0.4229\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4316 - val_loss: 0.4234\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4305 - val_loss: 0.4236\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4292 - val_loss: 0.4226\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4284 - val_loss: 0.4236\n",
      "121/121 [==============================] - 0s 708us/step - loss: 0.4490\n",
      "[CV] END learning_rate=0.0003308720455742883, n_hidden=2, n_neurons=74; total time=  21.3s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 3.4854 - val_loss: 1.9720\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.6872 - val_loss: 1.1850\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1824 - val_loss: 0.9406\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9703 - val_loss: 0.8258\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8635 - val_loss: 0.7642\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8022 - val_loss: 0.7244\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7621 - val_loss: 0.6959\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7339 - val_loss: 0.6748\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7119 - val_loss: 0.6570\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6940 - val_loss: 0.6413\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6787 - val_loss: 0.6271\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6650 - val_loss: 0.6136\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6527 - val_loss: 0.6028\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6418 - val_loss: 0.5923\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6315 - val_loss: 0.5821\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6219 - val_loss: 0.5717\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6132 - val_loss: 0.5644\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6045 - val_loss: 0.5538\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5972 - val_loss: 0.5475\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5896 - val_loss: 0.5410\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5825 - val_loss: 0.5343\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5758 - val_loss: 0.5266\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5691 - val_loss: 0.5197\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5630 - val_loss: 0.5143\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5569 - val_loss: 0.5090\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5512 - val_loss: 0.5033\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5455 - val_loss: 0.4974\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5402 - val_loss: 0.4937\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5350 - val_loss: 0.4893\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5300 - val_loss: 0.4845\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5252 - val_loss: 0.4799\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5206 - val_loss: 0.4766\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5160 - val_loss: 0.4747\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5119 - val_loss: 0.4683\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5078 - val_loss: 0.4654\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5039 - val_loss: 0.4633\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5001 - val_loss: 0.4609\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4965 - val_loss: 0.4590\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4931 - val_loss: 0.4555\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4898 - val_loss: 0.4532\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4866 - val_loss: 0.4504\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4836 - val_loss: 0.4499\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4806 - val_loss: 0.4477\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4778 - val_loss: 0.4472\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4750 - val_loss: 0.4479\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4726 - val_loss: 0.4445\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4699 - val_loss: 0.4449\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4676 - val_loss: 0.4441\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4652 - val_loss: 0.4417\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4630 - val_loss: 0.4427\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4608 - val_loss: 0.4406\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4588 - val_loss: 0.4408\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4568 - val_loss: 0.4405\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4549 - val_loss: 0.4419\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4530 - val_loss: 0.4415\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4512 - val_loss: 0.4416\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4494 - val_loss: 0.4415\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4478 - val_loss: 0.4429\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4461 - val_loss: 0.4420\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4446 - val_loss: 0.4417\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4431 - val_loss: 0.4436\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4415 - val_loss: 0.4447\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4401 - val_loss: 0.4449\n",
      "121/121 [==============================] - 0s 742us/step - loss: 0.4524\n",
      "[CV] END learning_rate=0.0003308720455742883, n_hidden=2, n_neurons=74; total time=  18.2s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 3.5849 - val_loss: 2.0018\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.5444 - val_loss: 1.0962\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9840 - val_loss: 0.8760\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8275 - val_loss: 0.8007\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7684 - val_loss: 0.7580\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7361 - val_loss: 0.7208\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7133 - val_loss: 0.6934\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6948 - val_loss: 0.6702\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6789 - val_loss: 0.6498\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6649 - val_loss: 0.6315\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6524 - val_loss: 0.6150\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6410 - val_loss: 0.6017\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6304 - val_loss: 0.5883\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6206 - val_loss: 0.5768\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6114 - val_loss: 0.5651\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6027 - val_loss: 0.5555\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5945 - val_loss: 0.5475\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5868 - val_loss: 0.5364\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5797 - val_loss: 0.5300\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5729 - val_loss: 0.5222\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5665 - val_loss: 0.5156\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5602 - val_loss: 0.5103\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5546 - val_loss: 0.5029\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5491 - val_loss: 0.4970\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5438 - val_loss: 0.4912\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5388 - val_loss: 0.4873\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5341 - val_loss: 0.4822\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5296 - val_loss: 0.4785\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5254 - val_loss: 0.4745\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5213 - val_loss: 0.4713\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5173 - val_loss: 0.4669\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5136 - val_loss: 0.4646\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5101 - val_loss: 0.4617\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5068 - val_loss: 0.4577\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5035 - val_loss: 0.4552\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5005 - val_loss: 0.4523\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4975 - val_loss: 0.4497\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4947 - val_loss: 0.4479\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4919 - val_loss: 0.4464\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4893 - val_loss: 0.4439\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4868 - val_loss: 0.4416\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4843 - val_loss: 0.4414\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4821 - val_loss: 0.4396\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4797 - val_loss: 0.4390\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4777 - val_loss: 0.4366\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4755 - val_loss: 0.4360\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4735 - val_loss: 0.4341\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4715 - val_loss: 0.4348\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4696 - val_loss: 0.4321\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4678 - val_loss: 0.4321\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4660 - val_loss: 0.4308\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4643 - val_loss: 0.4294\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4626 - val_loss: 0.4292\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4610 - val_loss: 0.4296\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4594 - val_loss: 0.4293\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4579 - val_loss: 0.4302\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4565 - val_loss: 0.4292\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4551 - val_loss: 0.4297\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4536 - val_loss: 0.4282\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4523 - val_loss: 0.4294\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4509 - val_loss: 0.4276\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4498 - val_loss: 0.4297\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4485 - val_loss: 0.4291\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4472 - val_loss: 0.4287\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4461 - val_loss: 0.4300\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4449 - val_loss: 0.4322\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4438 - val_loss: 0.4302\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4427 - val_loss: 0.4320\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4416 - val_loss: 0.4319\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4405 - val_loss: 0.4332\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4395 - val_loss: 0.4336\n",
      "121/121 [==============================] - 0s 755us/step - loss: 0.4291\n",
      "[CV] END learning_rate=0.0003308720455742883, n_hidden=2, n_neurons=74; total time=  20.7s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8120 - val_loss: 0.5410\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5180 - val_loss: 0.4365\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4578 - val_loss: 0.4402\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4239 - val_loss: 0.5092\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4039 - val_loss: 0.5532\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3910 - val_loss: 0.6185\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3817 - val_loss: 0.7074\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3761 - val_loss: 0.6938\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3697 - val_loss: 0.7972\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3641 - val_loss: 0.9210\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3590 - val_loss: 0.9439\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3534 - val_loss: 0.9472\n",
      "121/121 [==============================] - 0s 717us/step - loss: 0.3811\n",
      "[CV] END learning_rate=0.007126769280372353, n_hidden=3, n_neurons=45; total time=   3.9s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.9790 - val_loss: 0.6467\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5630 - val_loss: 0.4873\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4967 - val_loss: 0.4381\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4632 - val_loss: 0.4737\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4254 - val_loss: 0.4838\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4014 - val_loss: 0.5408\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3889 - val_loss: 0.5843\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3759 - val_loss: 0.6715\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3700 - val_loss: 0.7320\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3637 - val_loss: 0.7748\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3568 - val_loss: 0.8215\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3511 - val_loss: 0.8573\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3453 - val_loss: 0.9613\n",
      "121/121 [==============================] - 0s 783us/step - loss: 0.3533\n",
      "[CV] END learning_rate=0.007126769280372353, n_hidden=3, n_neurons=45; total time=   4.3s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.9469 - val_loss: 0.5662\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5366 - val_loss: 0.4629\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4740 - val_loss: 0.4534\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4399 - val_loss: 0.4709\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4178 - val_loss: 0.5206\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4076 - val_loss: 0.5745\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3951 - val_loss: 0.6272\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3857 - val_loss: 0.6740\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3778 - val_loss: 0.7638\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3691 - val_loss: 0.8132\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3646 - val_loss: 0.8566\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3616 - val_loss: 0.8652\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3536 - val_loss: 0.9764\n",
      "121/121 [==============================] - 0s 742us/step - loss: 0.3553\n",
      "[CV] END learning_rate=0.007126769280372353, n_hidden=3, n_neurons=45; total time=   4.7s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.9660 - val_loss: 0.7001\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6763 - val_loss: 0.5395\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5705 - val_loss: 0.4910\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5175 - val_loss: 0.4587\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4796 - val_loss: 0.4493\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4504 - val_loss: 0.4588\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4299 - val_loss: 0.4689\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4151 - val_loss: 0.4915\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4024 - val_loss: 0.5142\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3927 - val_loss: 0.5572\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3848 - val_loss: 0.5777\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3775 - val_loss: 0.5981\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3708 - val_loss: 0.6056\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3663 - val_loss: 0.6941\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3613 - val_loss: 0.6785\n",
      "121/121 [==============================] - 0s 730us/step - loss: 0.3814\n",
      "[CV] END learning_rate=0.0028951893171465437, n_hidden=3, n_neurons=83; total time=   5.4s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.3171 - val_loss: 0.7020\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6662 - val_loss: 0.5499\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5687 - val_loss: 0.5007\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5247 - val_loss: 0.4622\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4932 - val_loss: 0.4492\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4705 - val_loss: 0.4378\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4539 - val_loss: 0.4254\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4394 - val_loss: 0.4418\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4291 - val_loss: 0.4363\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4197 - val_loss: 0.4625\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4102 - val_loss: 0.4591\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4028 - val_loss: 0.4785\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3956 - val_loss: 0.4925\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3910 - val_loss: 0.5468\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3863 - val_loss: 0.5411\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3827 - val_loss: 0.5822\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3781 - val_loss: 0.5876\n",
      "121/121 [==============================] - 0s 739us/step - loss: 0.3902\n",
      "[CV] END learning_rate=0.0028951893171465437, n_hidden=3, n_neurons=83; total time=   5.7s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.4138 - val_loss: 0.6759\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6579 - val_loss: 0.5664\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5840 - val_loss: 0.4991\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5316 - val_loss: 0.4609\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4934 - val_loss: 0.4536\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4666 - val_loss: 0.4303\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4468 - val_loss: 0.4332\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4317 - val_loss: 0.4435\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4213 - val_loss: 0.4543\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4104 - val_loss: 0.4882\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4047 - val_loss: 0.4900\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3965 - val_loss: 0.4967\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3899 - val_loss: 0.5303\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3853 - val_loss: 0.5854\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3803 - val_loss: 0.6159\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3759 - val_loss: 0.6345\n",
      "121/121 [==============================] - 0s 783us/step - loss: 0.3651\n",
      "[CV] END learning_rate=0.0028951893171465437, n_hidden=3, n_neurons=83; total time=   5.6s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 0.6704 - val_loss: 0.6452\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6290 - val_loss: 2.0512\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8249 - val_loss: 6.1832\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 733us/step - loss: nan\n",
      "[CV] END learning_rate=0.02835909027838403, n_hidden=1, n_neurons=28; total time=   3.4s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 0.9468 - val_loss: 1.7285\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.7935 - val_loss: 5400.8125\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 725us/step - loss: nan\n",
      "[CV] END learning_rate=0.02835909027838403, n_hidden=1, n_neurons=28; total time=   3.4s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 0.6909 - val_loss: 1.0073\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7031 - val_loss: 1.3750\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6148 - val_loss: 0.5222\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4747 - val_loss: 0.5813\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4380 - val_loss: 0.6341\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4265 - val_loss: 0.9350\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4212 - val_loss: 0.8381\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4238 - val_loss: 1.0776\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4009 - val_loss: 1.0188\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3938 - val_loss: 1.1239\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4019 - val_loss: 1.1007\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3927 - val_loss: 1.0378\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3855 - val_loss: 1.1539\n",
      "121/121 [==============================] - 0s 709us/step - loss: 0.3710\n",
      "[CV] END learning_rate=0.02835909027838403, n_hidden=1, n_neurons=28; total time=   3.9s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.6987 - val_loss: 0.5583\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5236 - val_loss: 0.6680\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4457 - val_loss: 1.3084\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8934 - val_loss: 0.8470\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3718 - val_loss: 0.9077\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3612 - val_loss: 1.0916\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3503 - val_loss: 1.3385\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3488 - val_loss: 1.1565\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3405 - val_loss: 1.3225\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3347 - val_loss: 1.6489\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3317 - val_loss: 1.4475\n",
      "121/121 [==============================] - 0s 904us/step - loss: 0.3620\n",
      "[CV] END learning_rate=0.01674818428394745, n_hidden=3, n_neurons=29; total time=   4.0s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8158 - val_loss: 0.7718\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4672 - val_loss: 1.0903\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6974 - val_loss: 0.5373\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 733us/step - loss: nan\n",
      "[CV] END learning_rate=0.01674818428394745, n_hidden=3, n_neurons=29; total time=   4.6s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8185 - val_loss: 0.5699\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4821 - val_loss: 0.5273\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4387 - val_loss: 0.6182\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4058 - val_loss: 0.7537\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3876 - val_loss: 0.8548\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3794 - val_loss: 1.0602\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3777 - val_loss: 1.1475\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3641 - val_loss: 1.3696\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3597 - val_loss: 1.4438\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3523 - val_loss: 1.4204\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3513 - val_loss: 1.5497\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3515 - val_loss: 1.4611\n",
      "121/121 [==============================] - 0s 741us/step - loss: 0.3542\n",
      "[CV] END learning_rate=0.01674818428394745, n_hidden=3, n_neurons=29; total time=   4.0s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5978 - val_loss: 0.6969\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4288 - val_loss: 0.6379\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4557 - val_loss: 0.8666\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.5933 - val_loss: 1.1514\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 729us/step - loss: nan\n",
      "[CV] END learning_rate=0.02681276837118844, n_hidden=3, n_neurons=68; total time=   4.0s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.7235 - val_loss: 0.7940\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4351 - val_loss: 0.7609\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4606 - val_loss: 0.6281\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3812 - val_loss: 0.9904\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3699 - val_loss: 1.0784\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3539 - val_loss: 1.3741\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3488 - val_loss: 1.5282\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3437 - val_loss: 1.8146\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3331 - val_loss: 1.9129\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3259 - val_loss: 2.1757\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3224 - val_loss: 2.2167\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3174 - val_loss: 2.1646\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3100 - val_loss: 2.6656\n",
      "121/121 [==============================] - 0s 733us/step - loss: 0.3206\n",
      "[CV] END learning_rate=0.02681276837118844, n_hidden=3, n_neurons=68; total time=   4.6s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6212 - val_loss: 0.7460\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4190 - val_loss: 0.7176\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4032 - val_loss: 0.8817\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3801 - val_loss: 1.0404\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3694 - val_loss: 1.0218\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3635 - val_loss: 1.4258\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3605 - val_loss: 1.5488\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3410 - val_loss: 2.1820\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3350 - val_loss: 2.0513\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3271 - val_loss: 1.9977\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3286 - val_loss: 2.3749\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3225 - val_loss: 2.0912\n",
      "121/121 [==============================] - 0s 739us/step - loss: 0.3219\n",
      "[CV] END learning_rate=0.02681276837118844, n_hidden=3, n_neurons=68; total time=   4.1s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.1792 - val_loss: 0.6817\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6939 - val_loss: 0.5099\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5305 - val_loss: 0.4725\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4921 - val_loss: 0.4616\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4623 - val_loss: 0.4617\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4409 - val_loss: 0.4836\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4255 - val_loss: 0.4965\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4142 - val_loss: 0.5066\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4042 - val_loss: 0.5369\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3968 - val_loss: 0.5727\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3905 - val_loss: 0.5867\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3849 - val_loss: 0.6157\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3796 - val_loss: 0.6214\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3758 - val_loss: 0.6927\n",
      "121/121 [==============================] - 0s 716us/step - loss: 0.3953\n",
      "[CV] END learning_rate=0.005069771950367957, n_hidden=2, n_neurons=28; total time=   4.5s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.2810 - val_loss: 0.6860\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6492 - val_loss: 0.5392\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5641 - val_loss: 0.5080\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5380 - val_loss: 0.4531\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4988 - val_loss: 0.4551\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4678 - val_loss: 0.4362\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4460 - val_loss: 0.4397\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4306 - val_loss: 0.4615\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4198 - val_loss: 0.4668\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4148 - val_loss: 0.5030\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4016 - val_loss: 0.5132\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3960 - val_loss: 0.5313\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3894 - val_loss: 0.5583\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3861 - val_loss: 0.6201\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3819 - val_loss: 0.6245\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3784 - val_loss: 0.6586\n",
      "121/121 [==============================] - 0s 731us/step - loss: 0.3842\n",
      "[CV] END learning_rate=0.005069771950367957, n_hidden=2, n_neurons=28; total time=   5.0s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.0628 - val_loss: 0.6192\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6019 - val_loss: 0.5450\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5404 - val_loss: 0.4814\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4913 - val_loss: 0.4617\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4617 - val_loss: 0.5046\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4438 - val_loss: 0.4763\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4296 - val_loss: 0.4920\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4173 - val_loss: 0.5282\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4087 - val_loss: 0.5537\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4006 - val_loss: 0.6121\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3965 - val_loss: 0.6289\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3912 - val_loss: 0.6382\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3857 - val_loss: 0.6884\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3817 - val_loss: 0.7658\n",
      "121/121 [==============================] - 0s 725us/step - loss: 0.3751\n",
      "[CV] END learning_rate=0.005069771950367957, n_hidden=2, n_neurons=28; total time=   4.4s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.9522 - val_loss: 0.8290\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4110 - val_loss: 1.1298\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3748 - val_loss: 1.4419\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6549 - val_loss: 1.2290\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4009 - val_loss: 1.1460\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3717 - val_loss: 1.4500\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3345 - val_loss: 1.8279\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3472 - val_loss: 1.5459\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3476 - val_loss: 1.6530\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3310 - val_loss: 2.0476\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4035 - val_loss: 1.3868\n",
      "121/121 [==============================] - 0s 735us/step - loss: 0.3487\n",
      "[CV] END learning_rate=0.021452449765836837, n_hidden=3, n_neurons=75; total time=   3.8s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6940 - val_loss: 0.7499\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4279 - val_loss: 0.5249\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4244 - val_loss: 0.7120\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4188 - val_loss: 0.7707\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3765 - val_loss: 0.9819\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3597 - val_loss: 1.2259\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3565 - val_loss: 1.3070\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3488 - val_loss: 1.5835\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3396 - val_loss: 1.7122\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3327 - val_loss: 1.7627\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3290 - val_loss: 1.9013\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3250 - val_loss: 1.8816\n",
      "121/121 [==============================] - 0s 725us/step - loss: 0.3332\n",
      "[CV] END learning_rate=0.021452449765836837, n_hidden=3, n_neurons=75; total time=   4.0s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6332 - val_loss: 0.6307\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4264 - val_loss: 0.6296\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4175 - val_loss: 0.7809\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3909 - val_loss: 0.8540\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3677 - val_loss: 0.9326\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3578 - val_loss: 1.1615\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3555 - val_loss: 1.3317\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3418 - val_loss: 1.6447\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3333 - val_loss: 1.7504\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3247 - val_loss: 1.7404\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3251 - val_loss: 1.9100\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3227 - val_loss: 1.7385\n",
      "121/121 [==============================] - 0s 725us/step - loss: 0.3165\n",
      "[CV] END learning_rate=0.021452449765836837, n_hidden=3, n_neurons=75; total time=   4.1s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\miniconda3\\envs\\kdt_venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [-0.37476625 -0.39725231 -0.44349248 -0.36322306 -0.37891762         nan\n",
      "         nan         nan -0.38485162 -0.33280574]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 1s 1ms/step - loss: 0.6314 - val_loss: 0.4924\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4656 - val_loss: 0.8023\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5009 - val_loss: 0.5427\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4119 - val_loss: 0.8682\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7621 - val_loss: 0.5059\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6185 - val_loss: 0.7406\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3523 - val_loss: 0.6361\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3334 - val_loss: 0.5475\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3257 - val_loss: 0.4708\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3113 - val_loss: 0.7001\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3078 - val_loss: 0.7341\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3026 - val_loss: 0.6250\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2961 - val_loss: 0.7689\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2933 - val_loss: 0.8489\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2900 - val_loss: 0.6372\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2900 - val_loss: 0.9698\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2864 - val_loss: 1.5074\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2844 - val_loss: 1.1366\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2808 - val_loss: 0.9622\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=3,\n",
       "                   estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x000001717ACF14B0&gt;,\n",
       "                   param_distributions={&#x27;learning_rate&#x27;: [0.0007036992892745927,\n",
       "                                                          0.003638506452865738,\n",
       "                                                          0.01671140353898045,\n",
       "                                                          0.008741199405528767,\n",
       "                                                          0.012309588936393927,\n",
       "                                                          0.006232868841909281,\n",
       "                                                          0.0072723434950311555,\n",
       "                                                          0.014980025679402079,\n",
       "                                                          0.000947233987395861,\n",
       "                                                          0.002857400931720...\n",
       "                                                          0.0003507625445143638,\n",
       "                                                          0.00744666452597455,\n",
       "                                                          0.001179829550434225,\n",
       "                                                          0.021179399650598885,\n",
       "                                                          0.026256555079231938,\n",
       "                                                          0.02320888266687079,\n",
       "                                                          0.002664094880898269,\n",
       "                                                          0.015893024639660216,\n",
       "                                                          0.014662906045764888,\n",
       "                                                          0.0013041339510003749,\n",
       "                                                          0.013644329634570043, ...],\n",
       "                                        &#x27;n_hidden&#x27;: [0, 1, 2, 3],\n",
       "                                        &#x27;n_neurons&#x27;: [1, 2, 3, 4, 5, 6, 7, 8, 9,\n",
       "                                                      10, 11, 12, 13, 14, 15,\n",
       "                                                      16, 17, 18, 19, 20, 21,\n",
       "                                                      22, 23, 24, 25, 26, 27,\n",
       "                                                      28, 29, 30, ...]},\n",
       "                   verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=3,\n",
       "                   estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x000001717ACF14B0&gt;,\n",
       "                   param_distributions={&#x27;learning_rate&#x27;: [0.0007036992892745927,\n",
       "                                                          0.003638506452865738,\n",
       "                                                          0.01671140353898045,\n",
       "                                                          0.008741199405528767,\n",
       "                                                          0.012309588936393927,\n",
       "                                                          0.006232868841909281,\n",
       "                                                          0.0072723434950311555,\n",
       "                                                          0.014980025679402079,\n",
       "                                                          0.000947233987395861,\n",
       "                                                          0.002857400931720...\n",
       "                                                          0.0003507625445143638,\n",
       "                                                          0.00744666452597455,\n",
       "                                                          0.001179829550434225,\n",
       "                                                          0.021179399650598885,\n",
       "                                                          0.026256555079231938,\n",
       "                                                          0.02320888266687079,\n",
       "                                                          0.002664094880898269,\n",
       "                                                          0.015893024639660216,\n",
       "                                                          0.014662906045764888,\n",
       "                                                          0.0013041339510003749,\n",
       "                                                          0.013644329634570043, ...],\n",
       "                                        &#x27;n_hidden&#x27;: [0, 1, 2, 3],\n",
       "                                        &#x27;n_neurons&#x27;: [1, 2, 3, 4, 5, 6, 7, 8, 9,\n",
       "                                                      10, 11, 12, 13, 14, 15,\n",
       "                                                      16, 17, 18, 19, 20, 21,\n",
       "                                                      22, 23, 24, 25, 26, 27,\n",
       "                                                      28, 29, 30, ...]},\n",
       "                   verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x000001717ACF14B0&gt;</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x000001717ACF14B0&gt;</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=<keras.wrappers.scikit_learn.KerasRegressor object at 0x000001717ACF14B0>,\n",
       "                   param_distributions={'learning_rate': [0.0007036992892745927,\n",
       "                                                          0.003638506452865738,\n",
       "                                                          0.01671140353898045,\n",
       "                                                          0.008741199405528767,\n",
       "                                                          0.012309588936393927,\n",
       "                                                          0.006232868841909281,\n",
       "                                                          0.0072723434950311555,\n",
       "                                                          0.014980025679402079,\n",
       "                                                          0.000947233987395861,\n",
       "                                                          0.002857400931720...\n",
       "                                                          0.0003507625445143638,\n",
       "                                                          0.00744666452597455,\n",
       "                                                          0.001179829550434225,\n",
       "                                                          0.021179399650598885,\n",
       "                                                          0.026256555079231938,\n",
       "                                                          0.02320888266687079,\n",
       "                                                          0.002664094880898269,\n",
       "                                                          0.015893024639660216,\n",
       "                                                          0.014662906045764888,\n",
       "                                                          0.0013041339510003749,\n",
       "                                                          0.013644329634570043, ...],\n",
       "                                        'n_hidden': [0, 1, 2, 3],\n",
       "                                        'n_neurons': [1, 2, 3, 4, 5, 6, 7, 8, 9,\n",
       "                                                      10, 11, 12, 13, 14, 15,\n",
       "                                                      16, 17, 18, 19, 20, 21,\n",
       "                                                      22, 23, 24, 25, 26, 27,\n",
       "                                                      28, 29, 30, ...]},\n",
       "                   verbose=2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 하이퍼 파라미터가 많으므로 랜덤서치!\n",
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_hidden\": [0, 1, 2, 3],\n",
    "    \"n_neurons\": np.arange(1, 100).tolist(),\n",
    "    \"learning_rate\": reciprocal(3e-4, 3e-2).rvs(1000).tolist(),\n",
    "}\n",
    "\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3, verbose=2)       # k겹 교차검즘을 하므로 랜덤서치에서 valid_set은 안쓰인다.\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=100,                                                 # 다만 모델의 조기종료에서 사용 될 수 있다.\n",
    "                  validation_data=(X_valid, y_valid),\n",
    "                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'n_neurons': 75, 'n_hidden': 3, 'learning_rate': 0.021452449765836837},\n",
       " -0.33280574282010394)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_params_ , rnd_search_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x1717c258370>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = rnd_search_cv.best_estimator_.model\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_31\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_104 (Dense)           (None, 75)                675       \n",
      "                                                                 \n",
      " dense_105 (Dense)           (None, 75)                5700      \n",
      "                                                                 \n",
      " dense_106 (Dense)           (None, 75)                5700      \n",
      "                                                                 \n",
      " dense_107 (Dense)           (None, 1)                 76        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,151\n",
      "Trainable params: 12,151\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
