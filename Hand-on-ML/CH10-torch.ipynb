{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fashion_mnist데이터로 MLP구현 \\<Torch>\n",
    "- nn.NLLLoss() 와 nn.CrossEntropyLoss()의 차이\n",
    "- torch.nn.CrossEntropyLoss는 softmax + NLLLoss이다.\n",
    "    - 즉 , 모델의 출력층에 softmax()가 없는 모델에 사용해야한다.\n",
    "- 만약 지금처럼 model의 출력층에 LogSoftmax()가 있다면 NLLLoss()를 사용해야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tensorflow import keras\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# 데이터셋 적재\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full , y_train_full) , (X_test , y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# 정규화 값을 0 ~ 1사이로 맞추기\n",
    "X_valid , X_train = X_train_full[:5000] , X_train_full[5000:]\n",
    "y_valid , y_train = y_train_full[:5000] , y_train_full[5000:]\n",
    "X_train = X_train / 255.0\n",
    "X_valid = X_valid / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "X_valid , X_train = torch.tensor(X_valid,dtype=torch.float32) , torch.tensor(X_train,dtype=torch.float32)\n",
    "y_valid , y_train = torch.tensor(y_valid) , torch.tensor(y_train)\n",
    "X_test , y_test = torch.tensor(X_test,dtype=torch.float32) , torch.tensor(y_test)\n",
    "\n",
    "# 클래스 이름 리스트 정의\n",
    "class_names = [\"T-shirt\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle Boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터로더\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(TensorDataset(X_valid, y_valid), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두개의 은닉층으로 이루어진 MLP\n",
    "    - 텐서플로의 Dense는 토치의 Linear와 동일\n",
    "        - 단 Dense의 use_bias=True (default임)가 옵션이지만 Linear은 bias가 포함되어있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "이 방법은 계산 그래프를 생성하지 않고 순전파에서 모든 계산을 수행함\n",
    "따라서 파이토치의 자동미분 backward()를 수행할 때 참고할 계산 그래프가 없으므로 backward()를 사용할 수 없다.\n",
    "따라서 이방법은 사용하지 않는다!\n",
    "즉 , forward()에서 필요한 layer들은 미리 __init__에 선언해놓자!!!!!\n",
    "class Mlp_Net(nn.module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Mlp_Net,self).__init__()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = nn.ReLU(nn.Linear(28*28,300)(x.view(-1,28*28)))\n",
    "        x = nn.ReLU(nn.Linear(300,100)(x))\n",
    "        x = nn.Softmax(x,dim=1)\n",
    "        return x\n",
    "'''\n",
    "class Mlp_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layer1 = nn.Linear(28*28,300) # 입력 -> 출력 차원\n",
    "        self.layer2 = nn.Linear(300,100)\n",
    "        self.layer3 = nn.Linear(100,10)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mlp_Net(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (layer1): Linear(in_features=784, out_features=300, bias=True)\n",
      "  (layer2): Linear(in_features=300, out_features=100, bias=True)\n",
      "  (layer3): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Mlp_Net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1.weight torch.Size([300, 784]) torch.float32\n",
      "layer1.bias torch.Size([300]) torch.float32\n",
      "layer2.weight torch.Size([100, 300]) torch.float32\n",
      "layer2.bias torch.Size([100]) torch.float32\n",
      "layer3.weight torch.Size([10, 100]) torch.float32\n",
      "layer3.bias torch.Size([10]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "for name , param in model.named_parameters():\n",
    "    print(name , param.shape,param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(model.layer1.weight.shape,model.layer2.bias.shape)\\nprint(model.layer1.weight[0,0])\\nmodel.layer1.weight.data[0,0] = 0\\nprint(model.layer1.weight[0,0])\\n'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 파라미터에 직접 접근하기\n",
    "# 그래프에 직접 접근은 지양해야하며 .data로 접근할 수 있다.\n",
    "'''\n",
    "print(model.layer1.weight.shape,model.layer2.bias.shape)\n",
    "print(model.layer1.weight[0,0])\n",
    "model.layer1.weight.data[0,0] = 0\n",
    "print(model.layer1.weight[0,0])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile\n",
    "def train(model, train_loader, optimizer):\n",
    "    total_loss , total_acc = 0 , 0\n",
    "    for image, label in train_loader:\n",
    "        output = model(image)\n",
    "        loss = loss_fn(output, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss\n",
    "\n",
    "        pred = torch.max(output, 1)[1]\n",
    "        total_acc += (pred == label).sum()\n",
    "    return (total_loss / len(train_loader)) , (total_acc/len(train_loader.dataset))\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    total_loss , total_acc = 0 , 0\n",
    "    with torch.no_grad(): # 파라미터 업데이트 방지\n",
    "        for image, label in test_loader:\n",
    "            output = model(image)\n",
    "            loss = loss_fn(output,label)\n",
    "            total_loss += loss.item()\n",
    "            pred = torch.max(output,1)[1]\n",
    "            total_acc += (pred == label).sum()\n",
    "        return (total_loss / len(test_loader)) , (total_acc/len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1 train_loss : 1.11 val_acc : 64% val_loss : 0.66 val_acc : 76%\n",
      "epoch : 2 train_loss : 0.59 val_acc : 79% val_loss : 0.52 val_acc : 82%\n",
      "epoch : 3 train_loss : 0.51 val_acc : 82% val_loss : 0.47 val_acc : 84%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m epoch \u001b[39m=\u001b[39m \u001b[39m40\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m Epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m\u001b[39m+\u001b[39mepoch):\n\u001b[1;32m----> 9\u001b[0m     train_loss , train_acc\u001b[39m=\u001b[39m train(model, train_loader, optimizer)\n\u001b[0;32m     10\u001b[0m     test_loss, test_acc \u001b[39m=\u001b[39m evaluate(model, valid_loader)\n\u001b[0;32m     11\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mepoch : \u001b[39m\u001b[39m{\u001b[39;00mEpoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m train_loss : \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m val_acc : \u001b[39m\u001b[39m{\u001b[39;00mtrain_acc\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m.0f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m% val_loss : \u001b[39m\u001b[39m{\u001b[39;00mtest_loss\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m val_acc : \u001b[39m\u001b[39m{\u001b[39;00mtest_acc\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m.0f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[74], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, optimizer)\u001b[0m\n\u001b[0;32m      6\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(output, label)\n\u001b[0;32m      7\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m----> 8\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m      9\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     10\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n",
      "File \u001b[1;32mc:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 학습\n",
    "from tqdm import tqdm\n",
    "model = Mlp_Net()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "epoch = 40\n",
    "for Epoch in range(1+epoch):\n",
    "    train_loss , train_acc= train(model, train_loader, optimizer)\n",
    "    test_loss, test_acc = evaluate(model, valid_loader)\n",
    "    print(f'epoch : {Epoch+1} train_loss : {train_loss:.2f} val_acc : {train_acc*100:.0f}% val_loss : {test_loss:.2f} val_acc : {test_acc*100:.0f}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 안되서 긁어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "'''\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "trainset = datasets.FashionMNIST('./data',download=True, train= True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size= 64, shuffle=True)\n",
    "\n",
    "# Download and load test data\n",
    "testset = datasets.FashionMNIST('./data',download=True, train= False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size= 64, shuffle=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import NLLLoss\n",
    "from torch.optim import SGD\n",
    "\n",
    "model = nn.Sequential(\n",
    "nn.Flatten(),\n",
    "nn.Linear(28*28,300),\n",
    "nn.ReLU(),\n",
    "nn.Linear(300,100),\n",
    "nn.ReLU(),\n",
    "nn.Linear(100,10)\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = SGD(model.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mlp_Net(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (layer1): Linear(in_features=784, out_features=300, bias=True)\n",
      "  (layer2): Linear(in_features=300, out_features=100, bias=True)\n",
      "  (layer3): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1epoch Train_loss: 1.12 Train_acc: 63.01%|Test_loss: 0.67 Test_acc: 75.78%\n",
      "2epoch Train_loss: 0.61 Train_acc: 78.51%|Test_loss: 0.53 Test_acc: 81.82%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m loss \u001b[39m=\u001b[39m criterion(output,labels)\n\u001b[0;32m     11\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 12\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     13\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     15\u001b[0m pred \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(output, \u001b[39m1\u001b[39m)[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs =40\n",
    "model.to(device)\n",
    "for e in range(epochs):\n",
    "  total_loss , total_acc ,tot = 0 , 0 ,0\n",
    "  for images, labels in train_loader:\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    \n",
    "    output = model(images)\n",
    "    loss = criterion(output,labels)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    pred = torch.max(output, 1)[1].to(device)\n",
    "    total_loss += loss.item() * len(labels)\n",
    "    total_acc += (pred == labels).sum()\n",
    "  else:\n",
    "    print(f\"{e+1}epoch Train_loss: {total_loss/len(train_loader.dataset):.2f} Train_acc: {100*total_acc/len(train_loader.dataset):.2f}%\",end='|')\n",
    "\n",
    "\n",
    "  total_loss , total_acc = 0 , 0\n",
    "  with torch.no_grad():\n",
    "    for images, labels in valid_loader:\n",
    "      images, labels = images.to(device), labels.to(device)\n",
    "      output = model(images)\n",
    "      loss = criterion(output,labels)\n",
    "\n",
    "      pred = torch.max(output, 1)[1].to(device)\n",
    "      total_loss += loss.item() * len(labels)\n",
    "      total_acc += (pred == labels).sum()\n",
    "    else:\n",
    "      print(f\"Test_loss: {total_loss/len(valid_loader.dataset):.2f} Test_acc: {100*total_acc/len(valid_loader.dataset):.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 캘리포니아 주택 시작! MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11610, 8]) torch.Size([3870, 8]) torch.Size([5160, 8])\n",
      "torch.Size([11610, 1]) torch.Size([3870, 1]) torch.Size([5160, 1])\n"
     ]
    }
   ],
   "source": [
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full , X_test , y_train_full , y_test = train_test_split(housing.data,housing.target)\n",
    "X_train , X_valid , y_train , y_valid = train_test_split(X_train_full,y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.fit_transform(X_valid)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "X_train = torch.tensor(X_train,dtype=torch.float32)\n",
    "X_valid = torch.tensor(X_valid,dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test,dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train,dtype=torch.float32).view(-1,1)          # batch*차원으로 shape을 맞춰주기 위해\n",
    "y_valid = torch.tensor(y_valid,dtype=torch.float32).view(-1,1)\n",
    "y_test = torch.tensor(y_test,dtype=torch.float32).view(-1,1)\n",
    "\n",
    "print(X_train.shape , X_valid.shape , X_test.shape)\n",
    "print(y_train.shape , y_valid.shape , y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터로더\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(TensorDataset(X_valid, y_valid), batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,data_loader,valid_data_loader,epochs=10,criterion = nn.MSELoss):\n",
    "    for e in range(epochs):\n",
    "        total_train_loss = 0\n",
    "        for images, labels in data_loader:\n",
    "            output = model(images)\n",
    "            loss = criterion(output,labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "        print(f'{e+1}Epoch - loss : {(total_train_loss/len(data_loader)):.4f}',end=' ')\n",
    "        with torch.no_grad():\n",
    "            total_valid_loss = 0\n",
    "            for images, labels in valid_data_loader:\n",
    "                output = model(images)\n",
    "                loss = criterion(output,labels)\n",
    "                total_valid_loss += loss.item()\n",
    "            print(f'valid_loss : {(total_valid_loss/len(valid_data_loader)):.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1Epoch - loss : 1.0401 valid_loss : 0.6295\n",
      "2Epoch - loss : 0.5450 valid_loss : 0.4899\n",
      "3Epoch - loss : 0.4844 valid_loss : 0.5214\n",
      "4Epoch - loss : 0.4721 valid_loss : 0.5788\n",
      "5Epoch - loss : 0.4560 valid_loss : 0.7330\n",
      "6Epoch - loss : 0.4417 valid_loss : 0.7047\n",
      "7Epoch - loss : 0.4256 valid_loss : 0.8761\n",
      "8Epoch - loss : 0.4183 valid_loss : 1.0018\n",
      "9Epoch - loss : 0.4118 valid_loss : 1.1116\n",
      "10Epoch - loss : 0.4073 valid_loss : 1.3104\n",
      "11Epoch - loss : 0.4032 valid_loss : 1.4673\n",
      "12Epoch - loss : 0.4006 valid_loss : 1.6333\n",
      "13Epoch - loss : 0.4006 valid_loss : 1.8938\n",
      "14Epoch - loss : 0.3979 valid_loss : 1.9609\n",
      "15Epoch - loss : 0.3927 valid_loss : 2.1853\n",
      "16Epoch - loss : 0.3937 valid_loss : 2.2330\n",
      "17Epoch - loss : 0.3911 valid_loss : 2.4467\n",
      "18Epoch - loss : 0.3823 valid_loss : 2.6208\n",
      "19Epoch - loss : 0.3859 valid_loss : 2.8306\n",
      "20Epoch - loss : 0.3838 valid_loss : 3.1272\n"
     ]
    }
   ],
   "source": [
    "# 간단한 모델 만들기\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(8, 30),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30, 1)\n",
    ")\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.01)\n",
    "train(model,train_loader,valid_loader,20,criterion)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## skip connection 또는 residual connection 구현\n",
    "ResNet 등에서 사용되는 구조로, 이전 층의 출력을 현재 층의 입력에 더하여 정보의 유실을 방지하고, 학습의 안정성을 높이는 효과가 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1Epoch - loss : 0.9443 valid_loss : 0.8979\n",
      "2Epoch - loss : 0.6804 valid_loss : 1.0330\n",
      "3Epoch - loss : 0.6243 valid_loss : 0.6081\n",
      "4Epoch - loss : 0.6898 valid_loss : 0.5121\n",
      "5Epoch - loss : 0.5706 valid_loss : 0.4810\n",
      "6Epoch - loss : 0.4971 valid_loss : 0.4645\n",
      "7Epoch - loss : 0.4836 valid_loss : 0.4693\n",
      "8Epoch - loss : 0.4785 valid_loss : 0.4514\n",
      "9Epoch - loss : 0.4692 valid_loss : 0.4463\n",
      "10Epoch - loss : 0.4626 valid_loss : 0.4363\n",
      "11Epoch - loss : 0.4560 valid_loss : 0.4478\n",
      "12Epoch - loss : 0.4523 valid_loss : 0.4417\n",
      "13Epoch - loss : 0.4470 valid_loss : 0.4443\n",
      "14Epoch - loss : 0.4415 valid_loss : 0.4465\n",
      "15Epoch - loss : 0.4387 valid_loss : 0.4483\n",
      "16Epoch - loss : 0.4322 valid_loss : 0.4609\n",
      "17Epoch - loss : 0.4271 valid_loss : 0.5014\n",
      "18Epoch - loss : 0.4264 valid_loss : 0.4906\n",
      "19Epoch - loss : 0.4291 valid_loss : 0.5699\n",
      "20Epoch - loss : 0.4235 valid_loss : 0.5710\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(8, 30)\n",
    "        self.hidden2 = nn.Linear(30,30)\n",
    "        self.hidden3 = nn.Linear(38,1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.relu(self.hidden1(x))\n",
    "        x1 = self.relu(self.hidden2(x1))\n",
    "        x_concat = torch.cat((x,x1),dim=1)\n",
    "        result = self.hidden3(x_concat)\n",
    "        return result\n",
    "\n",
    "    \n",
    "model = Net()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.005)\n",
    "train(model,train_loader,valid_loader,20,criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1Epoch - loss : 2.1284 valid_loss : 1.0338\n",
      "2Epoch - loss : 0.8560 valid_loss : 0.7920\n",
      "3Epoch - loss : 0.7273 valid_loss : 0.7070\n",
      "4Epoch - loss : 0.6749 valid_loss : 0.6675\n",
      "5Epoch - loss : 0.6427 valid_loss : 0.6415\n",
      "6Epoch - loss : 0.6189 valid_loss : 0.6212\n",
      "7Epoch - loss : 0.5987 valid_loss : 0.6052\n",
      "8Epoch - loss : 0.5823 valid_loss : 0.5913\n",
      "9Epoch - loss : 0.5684 valid_loss : 0.5802\n",
      "10Epoch - loss : 0.5570 valid_loss : 0.5707\n",
      "11Epoch - loss : 0.5470 valid_loss : 0.5634\n",
      "12Epoch - loss : 0.5390 valid_loss : 0.5574\n",
      "13Epoch - loss : 0.5321 valid_loss : 0.5517\n",
      "14Epoch - loss : 0.5258 valid_loss : 0.5476\n",
      "15Epoch - loss : 0.5212 valid_loss : 0.5427\n",
      "16Epoch - loss : 0.5169 valid_loss : 0.5405\n",
      "17Epoch - loss : 0.5131 valid_loss : 0.5372\n",
      "18Epoch - loss : 0.5096 valid_loss : 0.5339\n",
      "19Epoch - loss : 0.5062 valid_loss : 0.5324\n",
      "20Epoch - loss : 0.5041 valid_loss : 0.5289\n"
     ]
    }
   ],
   "source": [
    "# 입력을 두개로 하는 모델 , 특성 나눈 후 넣기\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(5, 30)\n",
    "        self.hidden2 = nn.Linear(30,30)\n",
    "        self.hidden3 = nn.Linear(36,1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        input1 , input2 = x[:,:5] , x[:,2:]\n",
    "        x1 = self.relu(self.hidden1(input1))\n",
    "        x1 = self.relu(self.hidden2(x1))\n",
    "        concat = torch.cat((input2,x1),dim=1)\n",
    "        result = self.hidden3(concat)\n",
    "        return result\n",
    "\n",
    "    \n",
    "model = Net()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.001)\n",
    "train(model,train_loader,valid_loader,20,criterion)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여러 개의 출력\n",
    "- 예로들어 이미지에서 주요 물체를 분류하는 작업을 할 때 혹은 규제를 목적으로 사용됨\n",
    "    - 하위 layer가 상위 layer를 그대로 출력하나? 너무 의존하나 확인가능\n",
    "    - 얼굴사진으로 다중 작업 분류를 할 때\n",
    "        - 한 출력은 감정 분류\n",
    "        - 한 출력은 안경의 유무 확인\n",
    "- 여러개의 출력을 갖고 싶을 때 사용할 수 있다.\n",
    "- 이때 각 출력은 각자의 손실함수를 가지고 있어야 함!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1Epoch - loss : 1.2815 valid_loss : 1.0629\n",
      "2Epoch - loss : 1.0725 valid_loss : 0.9390\n",
      "3Epoch - loss : 0.8470 valid_loss : 0.9446\n",
      "4Epoch - loss : 0.7969 valid_loss : 0.8212\n",
      "5Epoch - loss : 0.7372 valid_loss : 0.7868\n",
      "6Epoch - loss : 0.7675 valid_loss : 0.7664\n",
      "7Epoch - loss : 0.6980 valid_loss : 0.6973\n",
      "8Epoch - loss : 0.6572 valid_loss : 0.7084\n",
      "9Epoch - loss : 0.6712 valid_loss : 0.6895\n",
      "10Epoch - loss : 0.6781 valid_loss : 0.7365\n",
      "11Epoch - loss : 0.7641 valid_loss : 0.6171\n",
      "12Epoch - loss : 0.7358 valid_loss : 3.8050\n",
      "13Epoch - loss : 0.6623 valid_loss : 0.6396\n",
      "14Epoch - loss : 0.9069 valid_loss : 0.6205\n",
      "15Epoch - loss : 0.6496 valid_loss : 0.6980\n",
      "16Epoch - loss : 1.0644 valid_loss : 0.6257\n",
      "17Epoch - loss : 0.6230 valid_loss : 0.6101\n",
      "18Epoch - loss : 0.6501 valid_loss : 0.6124\n",
      "19Epoch - loss : 0.6290 valid_loss : 0.7340\n",
      "20Epoch - loss : 0.6491 valid_loss : 1.2392\n"
     ]
    }
   ],
   "source": [
    "# 출력이 여러개인 모델 ,입력을 두개로 하는 모델 , 특성 나눈 후 넣기\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(5, 30)\n",
    "        self.hidden2 = nn.Linear(30,30)\n",
    "        self.hidden3 = nn.Linear(36,1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        input1 , input2 = x[:,:5] , x[:,2:]\n",
    "        x1 = self.relu(self.hidden1(input1))\n",
    "        x1 = self.relu(self.hidden2(x1))\n",
    "        concat = torch.cat((input2,x1),dim=1)\n",
    "        result = self.hidden3(concat)\n",
    "        return result , x1\n",
    "    \n",
    "    def train(self,data_loader,valid_data_loader,epochs=10,criterion = nn.MSELoss):\n",
    "        for e in range(epochs):\n",
    "            total_train_loss = 0\n",
    "            for images, labels in data_loader:\n",
    "                output1 , output2 = self(images)\n",
    "                loss1 , loss2 = criterion(output1,labels) , criterion(output2,labels)\n",
    "                loss = 0.9*loss1 + 0.1*loss2\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_train_loss += loss.item()\n",
    "            print(f'{e+1}Epoch - loss : {(total_train_loss/len(data_loader)):.4f}',end=' ')\n",
    "            with torch.no_grad():\n",
    "                total_valid_loss = 0\n",
    "                for images, labels in valid_data_loader:\n",
    "                    output1 , output2 = self(images)\n",
    "                    loss1 , loss2 = criterion(output1,labels) , criterion(output2,labels)\n",
    "                    loss = 0.9*loss1 + 0.1*loss2\n",
    "                    total_valid_loss += loss.item()\n",
    "                print(f'valid_loss : {(total_valid_loss/len(valid_data_loader)):.4f}')\n",
    "    \n",
    "model = Net()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.01)\n",
    "model.train(train_loader,valid_loader,20,criterion)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 저장 & 로드"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict\n",
    "1. 스크립트를 사용하면 모델을 고대로 저장하고 불러올 수 있다.\n",
    "2. 모델의 가중치만 저장하고 불러올 땐 모델을 정의하고 가중치만 불러올 수 있다.\n",
    "3. 모델의 에포크 등등 하이퍼 파라미터를 같이 저장 할 수 있다.\n",
    "    - Saving Multiple Models in One File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 스크립트를 사용하여 모델 저장\n",
    "model_scripted = torch.jit.script(model) # Export to TorchScript\n",
    "model_scripted.save('model_scripted.pt') # Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecursiveScriptModule(\n",
       "  original_name=Net\n",
       "  (hidden1): RecursiveScriptModule(original_name=Linear)\n",
       "  (hidden2): RecursiveScriptModule(original_name=Linear)\n",
       "  (hidden3): RecursiveScriptModule(original_name=Linear)\n",
       "  (relu): RecursiveScriptModule(original_name=ReLU)\n",
       ")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 새로운 모델로 불러오기\n",
    "model_new = torch.jit.load('model_scripted.pt')\n",
    "model_new.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (hidden1): Linear(in_features=5, out_features=30, bias=True)\n",
       "  (hidden2): Linear(in_features=30, out_features=30, bias=True)\n",
       "  (hidden3): Linear(in_features=36, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. 가중치를 저장 (불러올 때 모델 class를 정의해야 함)\n",
    "torch.save(model.state_dict(), 'weight.pt')\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(5, 30)\n",
    "        self.hidden2 = nn.Linear(30,30)\n",
    "        self.hidden3 = nn.Linear(36,1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        input1 , input2 = x[:,:5] , x[:,2:]\n",
    "        x1 = self.relu(self.hidden1(input1))\n",
    "        x1 = self.relu(self.hidden2(x1))\n",
    "        concat = torch.cat((input2,x1),dim=1)\n",
    "        result = self.hidden3(concat)\n",
    "        return result , x1\n",
    "model = Net()\n",
    "model.load_state_dict(torch.load('weight.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.Saving Multiple Models in One File\n",
    "# 여러 가지 값 저장, 학습 중 진행 상황 저장을 위해 epoch, loss 값 등 일반 scalar값 저장 가능\n",
    "\n",
    "torch.save({\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict()\n",
    "}, 'all.tar') \n",
    "checkpoint = torch.load('all.tar')   # dict 불러오기\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(5, 30)\n",
    "        self.hidden2 = nn.Linear(30,30)\n",
    "        self.hidden3 = nn.Linear(36,1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        input1 , input2 = x[:,:5] , x[:,2:]\n",
    "        x1 = self.relu(self.hidden1(input1))\n",
    "        x1 = self.relu(self.hidden2(x1))\n",
    "        concat = torch.cat((input2,x1),dim=1)\n",
    "        result = self.hidden3(concat)\n",
    "        return result , x1\n",
    "model_new = Net()\n",
    "\n",
    "model_new.load_state_dict(checkpoint['model'])\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.001)\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 콜백"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1Epoch - loss : 1.0764 valid_loss : 0.5549\n",
      "2Epoch - loss : 0.5853 valid_loss : 0.4941\n",
      "3Epoch - loss : 0.5370 valid_loss : 0.4672\n",
      "save ck\n",
      "4Epoch - loss : 0.5053 valid_loss : 0.4514\n",
      "save ck\n",
      "5Epoch - loss : 0.4912 valid_loss : 0.4854\n",
      "6Epoch - loss : 0.4878 valid_loss : 0.4717\n",
      "7Epoch - loss : 0.4755 valid_loss : 0.4716\n",
      "8Epoch - loss : 0.4650 valid_loss : 0.5141\n",
      "9Epoch - loss : 0.4577 valid_loss : 0.5240\n",
      "10Epoch - loss : 0.4521 valid_loss : 0.5604\n",
      "####Early stop####\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 최상의 모델을 저장 checkpoint 구현\n",
    "# 조기 종료\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(8, 30),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30, 1)\n",
    ")\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.005)\n",
    "def train(model,data_loader,valid_data_loader,epochs=10,patience=10,criterion = nn.MSELoss):\n",
    "    early_stop_cnt , pre_loss = 0 , float('inf')\n",
    "    for e in range(epochs):\n",
    "        total_train_loss = 0\n",
    "        for images, labels in data_loader:\n",
    "            output = model(images)\n",
    "            loss = criterion(output,labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "        train_loss = total_train_loss/len(data_loader)\n",
    "        print(f'{e+1}Epoch - loss : {train_loss:.4f}',end=' ')\n",
    "        with torch.no_grad():\n",
    "            total_valid_loss = 0\n",
    "            for images, labels in valid_data_loader:\n",
    "                output = model(images)\n",
    "                loss = criterion(output,labels)\n",
    "                total_valid_loss += loss.item()\n",
    "            valid_loss = total_valid_loss/len(valid_data_loader)\n",
    "            print(f'valid_loss : {valid_loss:.4f}')\n",
    "            try:\n",
    "                ck_loss = torch.load('ck.pt')['loss'] \n",
    "                if valid_loss < ck_loss:\n",
    "                    print('save ck')\n",
    "                    torch.save({\n",
    "                        'epoch' : e,\n",
    "                        'loss' : valid_loss,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                    },'ck.pt')\n",
    "            except:\n",
    "                torch.save({\n",
    "                        'epoch' : e,\n",
    "                        'loss' : valid_loss,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                    },'ck.pt')\n",
    "            if train_loss - pre_loss < 1e-3:\n",
    "                early_stop_cnt += 1\n",
    "                if early_stop_cnt == patience:\n",
    "                    torch.save({\n",
    "                        'epoch' : e,\n",
    "                        'loss' : valid_loss,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                    },'ck.pt')\n",
    "                    print(\"####Early stop####\")\n",
    "                    return 0\n",
    "            pre_loss = train_loss\n",
    "\n",
    "train(model,train_loader,valid_loader,epochs=50,patience=10,criterion=criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 0.5603854461642336)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('ck.pt')\n",
    "checkpoint['epoch'] , checkpoint['loss']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토치에서 텐서보드 활용하기\n",
    "- 기록은 디폴트로 .runs/에 저장됨\n",
    "- log_dir로 지정 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1Epoch - loss : 0.7962 valid_loss : 0.7518\n",
      "2Epoch - loss : 0.5816 valid_loss : 0.4435\n",
      "3Epoch - loss : 0.4953 valid_loss : 0.4918\n",
      "4Epoch - loss : 0.4648 valid_loss : 0.5159\n",
      "5Epoch - loss : 0.4443 valid_loss : 0.5603\n",
      "6Epoch - loss : 0.4379 valid_loss : 0.6495\n",
      "7Epoch - loss : 0.4292 valid_loss : 0.7422\n",
      "8Epoch - loss : 0.4230 valid_loss : 0.8221\n",
      "9Epoch - loss : 0.4144 valid_loss : 0.9591\n",
      "10Epoch - loss : 0.4088 valid_loss : 1.0896\n",
      "11Epoch - loss : 0.4035 valid_loss : 1.2331\n",
      "12Epoch - loss : 0.3976 valid_loss : 1.3666\n",
      "13Epoch - loss : 0.3942 valid_loss : 1.6192\n",
      "14Epoch - loss : 0.3928 valid_loss : 1.6868\n",
      "15Epoch - loss : 0.3886 valid_loss : 1.9327\n",
      "16Epoch - loss : 0.3907 valid_loss : 2.1442\n",
      "17Epoch - loss : 0.3821 valid_loss : 2.3347\n",
      "18Epoch - loss : 0.3805 valid_loss : 2.7081\n",
      "19Epoch - loss : 0.3800 valid_loss : 2.6961\n",
      "20Epoch - loss : 0.3791 valid_loss : 2.8549\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "root_logdir = os.path.join(os.curdir,\"my_logs\")\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime('run_%Y_%m_%d-%H_%M_%S')\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "run_logdir = get_run_logdir()\n",
    "writer = SummaryWriter(run_logdir)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(8, 30),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30, 1)\n",
    ")\n",
    "\n",
    "def train(model,data_loader,valid_data_loader,epochs=10,criterion = nn.MSELoss):\n",
    "    for e in range(epochs):\n",
    "        total_train_loss = 0\n",
    "        for images, labels in data_loader:\n",
    "            output = model(images)\n",
    "            loss = criterion(output,labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "        print(f'{e+1}Epoch - loss : {(total_train_loss/len(data_loader)):.4f}',end=' ')\n",
    "        with torch.no_grad():\n",
    "            total_valid_loss = 0\n",
    "            for images, labels in valid_data_loader:\n",
    "                output = model(images)\n",
    "                loss = criterion(output,labels)\n",
    "                total_valid_loss += loss.item()\n",
    "            print(f'valid_loss : {(total_valid_loss/len(valid_data_loader)):.4f}')\n",
    "        # 로그 저장\n",
    "        writer.add_scalar(\"Loss/train\", (total_train_loss/len(data_loader)), e)\n",
    "    writer.close()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.01)\n",
    "train(model,train_loader,valid_loader,20,criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-7366fc91de459f17\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-7366fc91de459f17\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./my_logs --port=6006"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
