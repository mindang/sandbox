{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fashion_mnist데이터로 MLP구현 \\<Torch>\n",
    "- nn.NLLLoss() 와 nn.CrossEntropyLoss()의 차이\n",
    "- torch.nn.CrossEntropyLoss는 softmax + NLLLoss이다.\n",
    "    - 즉 , 모델의 출력층에 softmax()가 없는 모델에 사용해야한다.\n",
    "- 만약 지금처럼 model의 출력층에 LogSoftmax()가 있다면 NLLLoss()를 사용해야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tensorflow import keras\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# 데이터셋 적재\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full , y_train_full) , (X_test , y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# 정규화 값을 0 ~ 1사이로 맞추기\n",
    "X_valid , X_train = X_train_full[:5000] , X_train_full[5000:]\n",
    "y_valid , y_train = y_train_full[:5000] , y_train_full[5000:]\n",
    "X_train = X_train / 255.0\n",
    "X_valid = X_valid / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "X_valid , X_train = torch.tensor(X_valid,dtype=torch.float32) , torch.tensor(X_train,dtype=torch.float32)\n",
    "y_valid , y_train = torch.tensor(y_valid) , torch.tensor(y_train)\n",
    "X_test , y_test = torch.tensor(X_test,dtype=torch.float32) , torch.tensor(y_test)\n",
    "\n",
    "# 클래스 이름 리스트 정의\n",
    "class_names = [\"T-shirt\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle Boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터로더\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(TensorDataset(X_valid, y_valid), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두개의 은닉층으로 이루어진 MLP\n",
    "    - 텐서플로의 Dense는 토치의 Linear와 동일\n",
    "        - 단 Dense의 use_bias=True (default임)가 옵션이지만 Linear은 bias가 포함되어있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "이 방법은 계산 그래프를 생성하지 않고 순전파에서 모든 계산을 수행함\n",
    "따라서 파이토치의 자동미분 backward()를 수행할 때 참고할 계산 그래프가 없으므로 backward()를 사용할 수 없다.\n",
    "따라서 이방법은 사용하지 않는다!\n",
    "즉 , forward()에서 필요한 layer들은 미리 __init__에 선언해놓자!!!!!\n",
    "class Mlp_Net(nn.module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Mlp_Net,self).__init__()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = nn.ReLU(nn.Linear(28*28,300)(x.view(-1,28*28)))\n",
    "        x = nn.ReLU(nn.Linear(300,100)(x))\n",
    "        x = nn.Softmax(x,dim=1)\n",
    "        return x\n",
    "'''\n",
    "class Mlp_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layer1 = nn.Linear(28*28,300) # 입력 -> 출력 차원\n",
    "        self.layer2 = nn.Linear(300,100)\n",
    "        self.layer3 = nn.Linear(100,10)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mlp_Net(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (layer1): Linear(in_features=784, out_features=300, bias=True)\n",
      "  (layer2): Linear(in_features=300, out_features=100, bias=True)\n",
      "  (layer3): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Mlp_Net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1.weight torch.Size([300, 784]) torch.float32\n",
      "layer1.bias torch.Size([300]) torch.float32\n",
      "layer2.weight torch.Size([100, 300]) torch.float32\n",
      "layer2.bias torch.Size([100]) torch.float32\n",
      "layer3.weight torch.Size([10, 100]) torch.float32\n",
      "layer3.bias torch.Size([10]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "for name , param in model.named_parameters():\n",
    "    print(name , param.shape,param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(model.layer1.weight.shape,model.layer2.bias.shape)\\nprint(model.layer1.weight[0,0])\\nmodel.layer1.weight.data[0,0] = 0\\nprint(model.layer1.weight[0,0])\\n'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 파라미터에 직접 접근하기\n",
    "# 그래프에 직접 접근은 지양해야하며 .data로 접근할 수 있다.\n",
    "'''\n",
    "print(model.layer1.weight.shape,model.layer2.bias.shape)\n",
    "print(model.layer1.weight[0,0])\n",
    "model.layer1.weight.data[0,0] = 0\n",
    "print(model.layer1.weight[0,0])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile\n",
    "def train(model, train_loader, optimizer):\n",
    "    total_loss , total_acc = 0 , 0\n",
    "    for image, label in train_loader:\n",
    "        output = model(image)\n",
    "        loss = loss_fn(output, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss\n",
    "\n",
    "        pred = torch.max(output, 1)[1]\n",
    "        total_acc += (pred == label).sum()\n",
    "    return (total_loss / len(train_loader)) , (total_acc/len(train_loader.dataset))\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    total_loss , total_acc = 0 , 0\n",
    "    with torch.no_grad(): # 파라미터 업데이트 방지\n",
    "        for image, label in test_loader:\n",
    "            output = model(image)\n",
    "            loss = loss_fn(output,label)\n",
    "            total_loss += loss.item()\n",
    "            pred = torch.max(output,1)[1]\n",
    "            total_acc += (pred == label).sum()\n",
    "        return (total_loss / len(test_loader)) , (total_acc/len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1 train_loss : 1.11 val_acc : 64% val_loss : 0.66 val_acc : 76%\n",
      "epoch : 2 train_loss : 0.59 val_acc : 79% val_loss : 0.52 val_acc : 82%\n",
      "epoch : 3 train_loss : 0.51 val_acc : 82% val_loss : 0.47 val_acc : 84%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m epoch \u001b[39m=\u001b[39m \u001b[39m40\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m Epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m\u001b[39m+\u001b[39mepoch):\n\u001b[1;32m----> 9\u001b[0m     train_loss , train_acc\u001b[39m=\u001b[39m train(model, train_loader, optimizer)\n\u001b[0;32m     10\u001b[0m     test_loss, test_acc \u001b[39m=\u001b[39m evaluate(model, valid_loader)\n\u001b[0;32m     11\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mepoch : \u001b[39m\u001b[39m{\u001b[39;00mEpoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m train_loss : \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m val_acc : \u001b[39m\u001b[39m{\u001b[39;00mtrain_acc\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m.0f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m% val_loss : \u001b[39m\u001b[39m{\u001b[39;00mtest_loss\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m val_acc : \u001b[39m\u001b[39m{\u001b[39;00mtest_acc\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m.0f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[74], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, optimizer)\u001b[0m\n\u001b[0;32m      6\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(output, label)\n\u001b[0;32m      7\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m----> 8\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m      9\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     10\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n",
      "File \u001b[1;32mc:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 학습\n",
    "from tqdm import tqdm\n",
    "model = Mlp_Net()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "epoch = 40\n",
    "for Epoch in range(1+epoch):\n",
    "    train_loss , train_acc= train(model, train_loader, optimizer)\n",
    "    test_loss, test_acc = evaluate(model, valid_loader)\n",
    "    print(f'epoch : {Epoch+1} train_loss : {train_loss:.2f} val_acc : {train_acc*100:.0f}% val_loss : {test_loss:.2f} val_acc : {test_acc*100:.0f}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 안되서 긁어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "'''\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "trainset = datasets.FashionMNIST('./data',download=True, train= True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size= 64, shuffle=True)\n",
    "\n",
    "# Download and load test data\n",
    "testset = datasets.FashionMNIST('./data',download=True, train= False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size= 64, shuffle=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import NLLLoss\n",
    "from torch.optim import SGD\n",
    "\n",
    "model = nn.Sequential(\n",
    "nn.Flatten(),\n",
    "nn.Linear(28*28,300),\n",
    "nn.ReLU(),\n",
    "nn.Linear(300,100),\n",
    "nn.ReLU(),\n",
    "nn.Linear(100,10)\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = SGD(model.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mlp_Net(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (layer1): Linear(in_features=784, out_features=300, bias=True)\n",
      "  (layer2): Linear(in_features=300, out_features=100, bias=True)\n",
      "  (layer3): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1epoch Train_loss: 1.12 Train_acc: 63.01%|Test_loss: 0.67 Test_acc: 75.78%\n",
      "2epoch Train_loss: 0.61 Train_acc: 78.51%|Test_loss: 0.53 Test_acc: 81.82%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m loss \u001b[39m=\u001b[39m criterion(output,labels)\n\u001b[0;32m     11\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 12\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     13\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     15\u001b[0m pred \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(output, \u001b[39m1\u001b[39m)[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\minki\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs =40\n",
    "model.to(device)\n",
    "for e in range(epochs):\n",
    "  total_loss , total_acc ,tot = 0 , 0 ,0\n",
    "  for images, labels in train_loader:\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    \n",
    "    output = model(images)\n",
    "    loss = criterion(output,labels)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    pred = torch.max(output, 1)[1].to(device)\n",
    "    total_loss += loss.item() * len(labels)\n",
    "    total_acc += (pred == labels).sum()\n",
    "  else:\n",
    "    print(f\"{e+1}epoch Train_loss: {total_loss/len(train_loader.dataset):.2f} Train_acc: {100*total_acc/len(train_loader.dataset):.2f}%\",end='|')\n",
    "\n",
    "\n",
    "  total_loss , total_acc = 0 , 0\n",
    "  with torch.no_grad():\n",
    "    for images, labels in valid_loader:\n",
    "      images, labels = images.to(device), labels.to(device)\n",
    "      output = model(images)\n",
    "      loss = criterion(output,labels)\n",
    "\n",
    "      pred = torch.max(output, 1)[1].to(device)\n",
    "      total_loss += loss.item() * len(labels)\n",
    "      total_acc += (pred == labels).sum()\n",
    "    else:\n",
    "      print(f\"Test_loss: {total_loss/len(valid_loader.dataset):.2f} Test_acc: {100*total_acc/len(valid_loader.dataset):.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 캘리포니아 주택 시작! MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11610, 8]) torch.Size([3870, 8]) torch.Size([5160, 8])\n",
      "torch.Size([11610]) torch.Size([3870]) torch.Size([5160])\n"
     ]
    }
   ],
   "source": [
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full , X_test , y_train_full , y_test = train_test_split(housing.data,housing.target)\n",
    "X_train , X_valid , y_train , y_valid = train_test_split(X_train_full,y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.fit_transform(X_valid)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "X_train = torch.tensor(X_train,dtype=torch.float32)\n",
    "X_valid = torch.tensor(X_valid,dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test,dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train,dtype=torch.float32)\n",
    "y_valid = torch.tensor(y_valid,dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test,dtype=torch.float32)\n",
    "\n",
    "print(X_train.shape , X_valid.shape , X_test.shape)\n",
    "print(y_train.shape , y_valid.shape , y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터로더\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(TensorDataset(X_valid, y_valid), batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,data_loader,epochs=10,criterion = torch.nn.MSELoss):\n",
    "    for e in range(epochs):\n",
    "        total_train_loss , total_ = 0\n",
    "        for images, labels in data_loader:\n",
    "            output = model(images)\n",
    "            loss = criterion(output,labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    total_loss += loss.item() * len(labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def evaluate(model,data_loader,epochs=10,criterion = torch.nn.MSELoss):\n",
    "    for e in range(epochs):\n",
    "        total_loss = 0\n",
    "        for images, labels in data_loader:\n",
    "            output = model(images)\n",
    "            loss = criterion(output,labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    total_loss += loss.item() * len(labels)\n",
    "    return total_loss / len(data_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11610"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
